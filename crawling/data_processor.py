#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬ í†µí•© ëª¨ë“ˆ
í¬ë¡¤ë§ëœ ë°ì´í„°ì˜ í…ìŠ¤íŠ¸ ì •ì œ, ê²€ì¦, ì €ì¥ì„ ë‹´ë‹¹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from text_cleaner import TextCleaner
from database import DatabaseManager
import logging
from typing import Dict, List
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataProcessor:
    """ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.text_cleaner = TextCleaner()
        self.db_manager = DatabaseManager()
        
        # ì •ì œ í†µê³„
        self.processing_stats = {
            "total_processed": 0,
            "successful_cleanings": 0,
            "failed_cleanings": 0,
            "validation_passed": 0,
            "validation_failed": 0
        }
    
    def process_job_posting(self, job_data: Dict) -> Dict:
        """ì±„ìš©ê³µê³  ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬"""
        logger.info(f"ì±„ìš©ê³µê³  ì •ì œ ì‹œì‘: {job_data.get('title', 'Unknown')}")
        
        processed_data = job_data.copy()
        self.processing_stats["total_processed"] += 1
        
        try:
            # í…ìŠ¤íŠ¸ í•„ë“œë“¤ ì •ì œ
            text_fields = [
                'title', 'company', 'location', 'salary', 'employment_type',
                'experience', 'education', 'main_duties', 'qualifications', 'preferences'
            ]
            
            cleaning_results = {}
            
            for field in text_fields:
                if field in processed_data and processed_data[field]:
                    original_text = str(processed_data[field])
                    
                    # í…ìŠ¤íŠ¸ ì •ì œ
                    cleaned_text = self.text_cleaner.clean_job_posting_text(original_text)
                    
                    # í’ˆì§ˆ ê²€ì¦ (í•„ë“œ íƒ€ì… ê³ ë ¤)
                    validation = self.text_cleaner.validate_cleaned_text(original_text, cleaned_text, field)
                    
                    if validation.get('valid', False):
                        processed_data[field] = cleaned_text
                        cleaning_results[field] = {
                            'success': True,
                            'original_length': len(original_text),
                            'cleaned_length': len(cleaned_text),
                            'length_ratio': validation.get('length_ratio', 0)
                        }
                    else:
                        logger.warning(f"í•„ë“œ {field} ì •ì œ ì‹¤íŒ¨: {validation.get('reason', 'Unknown')}")
                        cleaning_results[field] = {
                            'success': False,
                            'reason': validation.get('reason', 'Validation failed')
                        }
            
            # ì •ì œ ê²°ê³¼ í†µê³„ ì—…ë°ì´íŠ¸
            successful_fields = sum(1 for result in cleaning_results.values() if result.get('success'))
            total_fields = len(cleaning_results)
            
            if successful_fields > 0:
                self.processing_stats["successful_cleanings"] += 1
                
                if successful_fields == total_fields:
                    self.processing_stats["validation_passed"] += 1
                else:
                    self.processing_stats["validation_failed"] += 1
                    
                # ì •ì œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                processed_data['_cleaning_meta'] = {
                    'processed_fields': successful_fields,
                    'total_fields': total_fields,
                    'success_rate': round(successful_fields / total_fields, 2),
                    'cleaning_results': cleaning_results
                }
                
            else:
                self.processing_stats["failed_cleanings"] += 1
                logger.error(f"ì±„ìš©ê³µê³  ì •ì œ ì™„ì „ ì‹¤íŒ¨: {job_data.get('title', 'Unknown')}")
                
            return processed_data
            
        except Exception as e:
            logger.error(f"ì±„ìš©ê³µê³  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.processing_stats["failed_cleanings"] += 1
            return job_data
    
    def process_cover_letter(self, cover_letter_data: Dict) -> Dict:
        """ìê¸°ì†Œê°œì„œ ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬"""
        logger.info(f"ìê¸°ì†Œê°œì„œ ì •ì œ ì‹œì‘: {cover_letter_data.get('title', 'Unknown')}")
        
        processed_data = cover_letter_data.copy()
        self.processing_stats["total_processed"] += 1
        
        try:
            # í…ìŠ¤íŠ¸ í•„ë“œë“¤ ì •ì œ
            text_fields = [
                'title', 'company', 'position', 'department', 
                'experience_level', 'content'
            ]
            
            cleaning_results = {}
            
            for field in text_fields:
                if field in processed_data and processed_data[field]:
                    original_text = str(processed_data[field])
                    
                    # ìê¸°ì†Œê°œì„œ ì „ìš© ì •ì œ
                    cleaned_text = self.text_cleaner.clean_cover_letter_text(original_text)
                    
                    # í’ˆì§ˆ ê²€ì¦ (í•„ë“œ íƒ€ì… ê³ ë ¤)
                    validation = self.text_cleaner.validate_cleaned_text(original_text, cleaned_text, field)
                    
                    if validation.get('valid', False):
                        processed_data[field] = cleaned_text
                        cleaning_results[field] = {
                            'success': True,
                            'original_length': len(original_text),
                            'cleaned_length': len(cleaned_text),
                            'length_ratio': validation.get('length_ratio', 0)
                        }
                    else:
                        logger.warning(f"í•„ë“œ {field} ì •ì œ ì‹¤íŒ¨: {validation.get('reason', 'Unknown')}")
                        cleaning_results[field] = {
                            'success': False,
                            'reason': validation.get('reason', 'Validation failed')
                        }
            
            # í‚¤ì›Œë“œ ì •ì œ (ë°°ì—´ í•„ë“œ)
            if 'keywords' in processed_data and processed_data['keywords']:
                cleaned_keywords = []
                for keyword in processed_data['keywords']:
                    if keyword:
                        cleaned_keyword = self.text_cleaner.clean_cover_letter_text(str(keyword))
                        if cleaned_keyword and len(cleaned_keyword.strip()) > 1:
                            cleaned_keywords.append(cleaned_keyword.strip())
                
                processed_data['keywords'] = cleaned_keywords
            
            # ì •ì œ ê²°ê³¼ í†µê³„ ì—…ë°ì´íŠ¸
            successful_fields = sum(1 for result in cleaning_results.values() if result.get('success'))
            total_fields = len(cleaning_results)
            
            if successful_fields > 0:
                self.processing_stats["successful_cleanings"] += 1
                
                if successful_fields == total_fields:
                    self.processing_stats["validation_passed"] += 1
                else:
                    self.processing_stats["validation_failed"] += 1
                    
                # ì •ì œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                processed_data['_cleaning_meta'] = {
                    'processed_fields': successful_fields,
                    'total_fields': total_fields,
                    'success_rate': round(successful_fields / total_fields, 2),
                    'cleaning_results': cleaning_results
                }
                
            else:
                self.processing_stats["failed_cleanings"] += 1
                logger.error(f"ìê¸°ì†Œê°œì„œ ì •ì œ ì™„ì „ ì‹¤íŒ¨: {cover_letter_data.get('title', 'Unknown')}")
                
            return processed_data
            
        except Exception as e:
            logger.error(f"ìê¸°ì†Œê°œì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.processing_stats["failed_cleanings"] += 1
            return cover_letter_data
    
    def batch_process_job_postings(self, limit: int = 50) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ì˜ ì±„ìš©ê³µê³  ë°°ì¹˜ ì •ì œ"""
        logger.info(f"ì±„ìš©ê³µê³  ë°°ì¹˜ ì •ì œ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì›ì‹œ ì±„ìš©ê³µê³  ë°ì´í„° ì¡°íšŒ
            self.db_manager.connect()
            
            query = """
            SELECT id, title, company, location, salary, employment_type,
                   experience, education, main_duties, qualifications, preferences,
                   url, source, created_at
            FROM mlops.job_postings
            WHERE main_duties IS NOT NULL OR qualifications IS NOT NULL
            ORDER BY created_at DESC
            LIMIT %s;
            """
            
            job_postings = self.db_manager.execute_query(query, (limit,), fetch=True)
            
            if not job_postings:
                logger.info("ì •ì œí•  ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {"processed": 0, "success": 0}
            
            logger.info(f"{len(job_postings)}ê°œì˜ ì±„ìš©ê³µê³ ë¥¼ ì •ì œí•©ë‹ˆë‹¤.")
            
            processed_count = 0
            success_count = 0
            
            for job in job_postings:
                # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                job_data = dict(job)
                
                # ì •ì œ ì²˜ë¦¬
                processed_job = self.process_job_posting(job_data)
                
                # ì •ì œ ë©”íƒ€ë°ì´í„° í™•ì¸
                if '_cleaning_meta' in processed_job:
                    success_rate = processed_job['_cleaning_meta'].get('success_rate', 0)
                    if success_rate > 0.5:  # 50% ì´ìƒ í•„ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì •ì œë¨
                        success_count += 1
                        
                        # ì •ì œëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ (ë©”íƒ€ë°ì´í„° ì œì™¸)
                        cleaned_job = {k: v for k, v in processed_job.items() if not k.startswith('_')}
                        self._update_job_posting(cleaned_job)
                
                processed_count += 1
                
                if processed_count % 10 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {processed_count}/{len(job_postings)}")
            
            result = {
                "processed": processed_count,
                "success": success_count,
                "success_rate": round(success_count / processed_count, 2) if processed_count > 0 else 0,
                "stats": self.processing_stats.copy()
            }
            
            logger.info(f"ë°°ì¹˜ ì •ì œ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬, {success_count}ê°œ ì„±ê³µ")
            return result
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"processed": 0, "success": 0, "error": str(e)}
        finally:
            self.db_manager.disconnect()
    
    def batch_process_cover_letters(self, limit: int = 50) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ì˜ ìê¸°ì†Œê°œì„œ ë°°ì¹˜ ì •ì œ"""
        logger.info(f"ìê¸°ì†Œê°œì„œ ë°°ì¹˜ ì •ì œ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì›ì‹œ ìê¸°ì†Œê°œì„œ ë°ì´í„° ì¡°íšŒ
            self.db_manager.connect()
            
            query = """
            SELECT id, title, company, position, department, experience_level,
                   content, keywords, url, source, created_at
            FROM mlops.cover_letter_samples
            WHERE content IS NOT NULL
            ORDER BY created_at DESC
            LIMIT %s;
            """
            
            cover_letters = self.db_manager.execute_query(query, (limit,), fetch=True)
            
            if not cover_letters:
                logger.info("ì •ì œí•  ìê¸°ì†Œê°œì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {"processed": 0, "success": 0}
            
            logger.info(f"{len(cover_letters)}ê°œì˜ ìê¸°ì†Œê°œì„œë¥¼ ì •ì œí•©ë‹ˆë‹¤.")
            
            processed_count = 0
            success_count = 0
            
            for cover_letter in cover_letters:
                # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                cover_letter_data = dict(cover_letter)
                
                # ì •ì œ ì²˜ë¦¬
                processed_letter = self.process_cover_letter(cover_letter_data)
                
                # ì •ì œ ë©”íƒ€ë°ì´í„° í™•ì¸
                if '_cleaning_meta' in processed_letter:
                    success_rate = processed_letter['_cleaning_meta'].get('success_rate', 0)
                    if success_rate > 0.5:  # 50% ì´ìƒ í•„ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì •ì œë¨
                        success_count += 1
                        
                        # ì •ì œëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ (ë©”íƒ€ë°ì´í„° ì œì™¸)
                        cleaned_letter = {k: v for k, v in processed_letter.items() if not k.startswith('_')}
                        self._update_cover_letter(cleaned_letter)
                
                processed_count += 1
                
                if processed_count % 5 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {processed_count}/{len(cover_letters)}")
            
            result = {
                "processed": processed_count,
                "success": success_count,
                "success_rate": round(success_count / processed_count, 2) if processed_count > 0 else 0,
                "stats": self.processing_stats.copy()
            }
            
            logger.info(f"ë°°ì¹˜ ì •ì œ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬, {success_count}ê°œ ì„±ê³µ")
            return result
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"processed": 0, "success": 0, "error": str(e)}
        finally:
            self.db_manager.disconnect()
    
    def _update_job_posting(self, job_data: Dict) -> bool:
        """ì •ì œëœ ì±„ìš©ê³µê³  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸"""
        try:
            update_query = """
            UPDATE mlops.job_postings SET
                title = %(title)s,
                company = %(company)s,
                location = %(location)s,
                salary = %(salary)s,
                employment_type = %(employment_type)s,
                experience = %(experience)s,
                education = %(education)s,
                main_duties = %(main_duties)s,
                qualifications = %(qualifications)s,
                preferences = %(preferences)s,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = %(id)s;
            """
            
            self.db_manager.execute_query(update_query, job_data, fetch=False)
            return True
            
        except Exception as e:
            logger.error(f"ì±„ìš©ê³µê³  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ID: {job_data.get('id')}): {e}")
            return False
    
    def _update_cover_letter(self, cover_letter_data: Dict) -> bool:
        """ì •ì œëœ ìê¸°ì†Œê°œì„œ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸"""
        try:
            update_query = """
            UPDATE mlops.cover_letter_samples SET
                title = %(title)s,
                company = %(company)s,
                position = %(position)s,
                department = %(department)s,
                experience_level = %(experience_level)s,
                content = %(content)s,
                keywords = %(keywords)s,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = %(id)s;
            """
            
            self.db_manager.execute_query(update_query, cover_letter_data, fetch=False)
            return True
            
        except Exception as e:
            logger.error(f"ìê¸°ì†Œê°œì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ID: {cover_letter_data.get('id')}): {e}")
            return False
    
    def get_processing_stats(self) -> Dict:
        """ì •ì œ ì²˜ë¦¬ í†µê³„ ì¡°íšŒ"""
        return self.processing_stats.copy()

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    processor = DataProcessor()
    
    print("ğŸ”§ ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ì±„ìš©ê³µê³  ë°°ì¹˜ ì •ì œ
    print("\nğŸ“‹ ì±„ìš©ê³µê³  ë°°ì¹˜ ì •ì œ ì‹œì‘...")
    job_result = processor.batch_process_job_postings(limit=5)
    
    print(f"âœ… ì±„ìš©ê³µê³  ì •ì œ ê²°ê³¼:")
    print(f"  - ì²˜ë¦¬ëœ ê°œìˆ˜: {job_result.get('processed', 0)}ê°œ")
    print(f"  - ì„±ê³µí•œ ê°œìˆ˜: {job_result.get('success', 0)}ê°œ")
    print(f"  - ì„±ê³µë¥ : {job_result.get('success_rate', 0):.1%}")
    
    # 2. ìê¸°ì†Œê°œì„œ ë°°ì¹˜ ì •ì œ
    print("\nğŸ“ ìê¸°ì†Œê°œì„œ ë°°ì¹˜ ì •ì œ ì‹œì‘...")
    letter_result = processor.batch_process_cover_letters(limit=5)
    
    print(f"âœ… ìê¸°ì†Œê°œì„œ ì •ì œ ê²°ê³¼:")
    print(f"  - ì²˜ë¦¬ëœ ê°œìˆ˜: {letter_result.get('processed', 0)}ê°œ")
    print(f"  - ì„±ê³µí•œ ê°œìˆ˜: {letter_result.get('success', 0)}ê°œ")
    print(f"  - ì„±ê³µë¥ : {letter_result.get('success_rate', 0):.1%}")
    
    # 3. ì „ì²´ í†µê³„
    stats = processor.get_processing_stats()
    print(f"\nğŸ“Š ì „ì²´ ì •ì œ í†µê³„:")
    print(f"  - ì´ ì²˜ë¦¬: {stats['total_processed']}ê°œ")
    print(f"  - ì •ì œ ì„±ê³µ: {stats['successful_cleanings']}ê°œ")
    print(f"  - ì •ì œ ì‹¤íŒ¨: {stats['failed_cleanings']}ê°œ")
    print(f"  - ê²€ì¦ í†µê³¼: {stats['validation_passed']}ê°œ")
    print(f"  - ê²€ì¦ ì‹¤íŒ¨: {stats['validation_failed']}ê°œ")

if __name__ == "__main__":
    main()