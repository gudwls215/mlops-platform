"""
í¬ë¡¤ë§ ë©”ì¸ ì‹¤í–‰ê¸°
"""
import argparse
import logging
import sys
from datetime import datetime
from pathlib import Path

from database import DatabaseManager
from scrapers.saramin_crawler import SaraminCrawler


def setup_logging(log_level: str = 'INFO'):
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('crawler.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='MLOps í”Œë«í¼ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬')
    parser.add_argument('--site', choices=['saramin', 'all'], default='all',
                       help='í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ì„ íƒ')
    parser.add_argument('--max-jobs', type=int, default=100,
                       help='ìµœëŒ€ í¬ë¡¤ë§í•  ì±„ìš©ê³µê³  ìˆ˜')
    parser.add_argument('--export-csv', type=str,
                       help='CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚¼ íŒŒì¼ëª…')
    parser.add_argument('--stats', action='store_true',
                       help='ì±„ìš©ê³µê³  í†µê³„ ì¶œë ¥')
    parser.add_argument('--cleanup', action='store_true',
                       help='ì˜¤ë˜ëœ ì±„ìš©ê³µê³  ì •ë¦¬')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       default='INFO', help='ë¡œê·¸ ë ˆë²¨')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    db_manager = DatabaseManager()
    
    try:
        db_manager.connect()
        logger.info("í¬ë¡¤ë§ ì‹œì‘")
        
        # í†µê³„ ì¶œë ¥
        if args.stats:
            print_statistics(db_manager)
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        if args.site in ['saramin', 'all']:
            crawl_saramin(db_manager, args.max_jobs)
        
        # ì •ë¦¬ ì‘ì—…
        if args.cleanup:
            db_manager.cleanup_old_postings(days=30)
        
        # CSV ë‚´ë³´ë‚´ê¸°
        if args.export_csv:
            db_manager.export_to_csv(args.export_csv)
        
        logger.info("í¬ë¡¤ë§ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)
        
    finally:
        db_manager.disconnect()


def crawl_saramin(db_manager: DatabaseManager, max_jobs: int):
    """ì‚¬ëŒì¸ í¬ë¡¤ë§"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ì‚¬ëŒì¸ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_jobs}ê°œ)")
        
        crawler = SaraminCrawler()
        jobs = crawler.crawl_jobs(max_jobs=max_jobs)
        
        if jobs:
            inserted_count = db_manager.bulk_insert_job_postings(jobs)
            logger.info(f"ì‚¬ëŒì¸ í¬ë¡¤ë§ ì™„ë£Œ: {inserted_count}ê°œ ì±„ìš©ê³µê³  ì €ì¥")
            
            # í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
            print_crawling_summary(jobs)
        else:
            logger.warning("ì‚¬ëŒì¸ì—ì„œ í¬ë¡¤ë§ëœ ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"ì‚¬ëŒì¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")


def print_statistics(db_manager: DatabaseManager):
    """í†µê³„ ì¶œë ¥"""
    stats = db_manager.get_job_posting_stats()
    
    print("\n" + "="*50)
    print("ğŸ“Š ì±„ìš©ê³µê³  í†µê³„")
    print("="*50)
    print(f"ì´ ì±„ìš©ê³µê³  ìˆ˜: {stats['total']:,}ê°œ")
    print(f"ìµœê·¼ 7ì¼ ë“±ë¡: {stats['recent']:,}ê°œ")
    
    if stats['by_site']:
        print("\nğŸŒ ì‚¬ì´íŠ¸ë³„ ë¶„í¬:")
        for item in stats['by_site']:
            print(f"  â€¢ {item['source']}: {item['count']:,}ê°œ")
    
    if stats['by_employment_type']:
        print("\nğŸ’¼ ê³ ìš©í˜•íƒœë³„ ë¶„í¬:")
        for item in stats['by_employment_type'][:5]:
            print(f"  â€¢ {item['employment_type']}: {item['count']:,}ê°œ")
    
    if stats['by_location']:
        print("\nğŸ“ ì§€ì—­ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        for item in stats['by_location'][:10]:
            location = item['location'] if item['location'] else 'ë¯¸ì§€ì •'
            print(f"  â€¢ {location}: {item['count']:,}ê°œ")
    
    print("="*50)


def print_crawling_summary(jobs):
    """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    if not jobs:
        return
    
    print(f"\nğŸ“‹ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
    print("-"*30)
    print(f"ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³ : {len(jobs)}ê°œ")
    
    # íšŒì‚¬ë³„ ë¶„í¬
    companies = {}
    for job in jobs:
        company = job.get('company', 'ë¯¸ì§€ì •')
        companies[company] = companies.get(company, 0) + 1
    
    print(f"\níšŒì‚¬ë³„ ë¶„í¬ (ìƒìœ„ 5ê°œ):")
    for company, count in sorted(companies.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"  â€¢ {company}: {count}ê°œ")
    
    # ì§€ì—­ë³„ ë¶„í¬
    locations = {}
    for job in jobs:
        location = job.get('location', 'ë¯¸ì§€ì •')
        if location:
            locations[location] = locations.get(location, 0) + 1
    
    if locations:
        print(f"\nì§€ì—­ë³„ ë¶„í¬ (ìƒìœ„ 5ê°œ):")
        for location, count in sorted(locations.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  â€¢ {location}: {count}ê°œ")
    
    # ê³ ìš©í˜•íƒœë³„ ë¶„í¬
    employment_types = {}
    for job in jobs:
        emp_type = job.get('employment_type', 'ë¯¸ì§€ì •')
        if emp_type:
            employment_types[emp_type] = employment_types.get(emp_type, 0) + 1
    
    if employment_types:
        print(f"\nê³ ìš©í˜•íƒœë³„ ë¶„í¬:")
        for emp_type, count in employment_types.items():
            print(f"  â€¢ {emp_type}: {count}ê°œ")
    
    print("-"*30)
    
    # ìƒìœ„ 3ê°œ ì±„ìš©ê³µê³  ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ” ì±„ìš©ê³µê³  ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 3ê°œ):")
    for i, job in enumerate(jobs[:3], 1):
        print(f"\n{i}. {job.get('title', 'N/A')}")
        print(f"   íšŒì‚¬: {job.get('company', 'N/A')}")
        print(f"   ì§€ì—­: {job.get('location', 'N/A')}")
        print(f"   ê¸‰ì—¬: {job.get('salary', 'N/A')}")
        print(f"   ê³ ìš©í˜•íƒœ: {job.get('employment_type', 'N/A')}")


if __name__ == "__main__":
    main()