#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ ëª¨ë“ˆ
HTML íƒœê·¸ ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬, í…ìŠ¤íŠ¸ ì •ê·œí™”
"""

import re
import html
import unicodedata
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextCleaner:
    """í…ìŠ¤íŠ¸ ì •ì œ ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # HTML íƒœê·¸ ì œê±°ìš© ì •ê·œí‘œí˜„ì‹
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        
        # íŠ¹ìˆ˜ë¬¸ì íŒ¨í„´ë“¤
        self.special_chars_pattern = re.compile(r'[^\w\sê°€-í£.,!?()[\]{}":;-]')
        self.multiple_spaces_pattern = re.compile(r'\s+')
        self.multiple_newlines_pattern = re.compile(r'\n{3,}')
        
        # ë¶ˆí•„ìš”í•œ ë¬¸ìì—´ íŒ¨í„´ë“¤
        self.unwanted_patterns = [
            re.compile(r'&[a-zA-Z]+;'),  # HTML ì—”í‹°í‹°
            re.compile(r'&#\d+;'),       # ìˆ«ìí˜• HTML ì—”í‹°í‹°
            re.compile(r'\[.*?\]'),      # ëŒ€ê´„í˜¸ ë‚´ìš©
            re.compile(r'<.*?>'),        # HTML íƒœê·¸
            re.compile(r'javascript:.*?;'), # JavaScript ì½”ë“œ
            re.compile(r'style=".*?"'),     # ì¸ë¼ì¸ ìŠ¤íƒ€ì¼
        ]
        
        # ì´ë©”ì¼ê³¼ URL íŒ¨í„´
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        
        # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ íŒ¨í„´
        self.korean_sentence_pattern = re.compile(r'[.!?]+\s+')
        
    def remove_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not text:
            return ""
        
        try:
            # BeautifulSoupì„ ì‚¬ìš©í•œ HTML íƒœê·¸ ì œê±°
            soup = BeautifulSoup(text, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì™„ì „ ì œê±°
            for script in soup(["script", "style", "meta", "link", "head"]):
                script.decompose()
            
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            clean_text = soup.get_text()
            
            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë‚¨ì€ íƒœê·¸ ì œê±°
            clean_text = self.html_tag_pattern.sub('', clean_text)
            
            return clean_text.strip()
            
        except Exception as e:
            logger.warning(f"HTML íƒœê·¸ ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            # BeautifulSoup ì‹¤íŒ¨ì‹œ ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
            return self.html_tag_pattern.sub('', text).strip()
    
    def decode_html_entities(self, text: str) -> str:
        """HTML ì—”í‹°í‹° ë””ì½”ë”©"""
        if not text:
            return ""
        
        try:
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            decoded_text = html.unescape(text)
            
            # ë‚¨ì€ HTML ì—”í‹°í‹° íŒ¨í„´ ì œê±°
            for pattern in self.unwanted_patterns[:2]:  # HTML ì—”í‹°í‹° íŒ¨í„´ë§Œ
                decoded_text = pattern.sub('', decoded_text)
            
            return decoded_text
            
        except Exception as e:
            logger.warning(f"HTML ì—”í‹°í‹° ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜: {e}")
            return text
    
    def normalize_unicode(self, text: str) -> str:
        """ìœ ë‹ˆì½”ë“œ ì •ê·œí™”"""
        if not text:
            return ""
        
        try:
            # NFKC ì •ê·œí™” (í˜¸í™˜ì„± ë¬¸ìë¥¼ ì •ê·œí˜•ìœ¼ë¡œ ë³€í™˜)
            normalized_text = unicodedata.normalize('NFKC', text)
            return normalized_text
            
        except Exception as e:
            logger.warning(f"ìœ ë‹ˆì½”ë“œ ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {e}")
            return text
    
    def remove_special_characters(self, text: str, preserve_punctuation: bool = True) -> str:
        """íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
        if not text:
            return ""
        
        try:
            if preserve_punctuation:
                # ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ëŠ” ë³´ì¡´í•˜ê³  ë‚˜ë¨¸ì§€ íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°
                clean_text = self.special_chars_pattern.sub('', text)
            else:
                # ëª¨ë“  íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ê³µë°±ë§Œ ìœ ì§€)
                clean_text = re.sub(r'[^\w\sê°€-í£]', '', text)
            
            return clean_text
            
        except Exception as e:
            logger.warning(f"íŠ¹ìˆ˜ë¬¸ì ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            return text
    
    def normalize_whitespace(self, text: str) -> str:
        """ê³µë°± ì •ê·œí™”"""
        if not text:
            return ""
        
        try:
            # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
            clean_text = self.multiple_spaces_pattern.sub(' ', text)
            
            # ì—°ì†ëœ ê°œí–‰ë¬¸ì ì •ë¦¬ (ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í—ˆìš©)
            clean_text = self.multiple_newlines_pattern.sub('\n\n', clean_text)
            
            # ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
            lines = clean_text.split('\n')
            clean_lines = [line.strip() for line in lines]
            
            # ë¹ˆ ì¤„ì´ 3ê°œ ì´ìƒ ì—°ì†ë˜ì§€ ì•Šë„ë¡ ì œí•œ
            result_lines = []
            empty_count = 0
            
            for line in clean_lines:
                if line == '':
                    empty_count += 1
                    if empty_count <= 2:  # ìµœëŒ€ 2ê°œì˜ ë¹ˆ ì¤„ë§Œ í—ˆìš©
                        result_lines.append(line)
                else:
                    empty_count = 0
                    result_lines.append(line)
            
            return '\n'.join(result_lines).strip()
            
        except Exception as e:
            logger.warning(f"ê³µë°± ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {e}")
            return text.strip()
    
    def remove_unwanted_patterns(self, text: str) -> str:
        """ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°"""
        if not text:
            return ""
        
        try:
            clean_text = text
            
            # URL ì œê±° (ì„ íƒì )
            clean_text = self.url_pattern.sub('[URL]', clean_text)
            
            # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹ (ì„ íƒì )
            clean_text = self.email_pattern.sub('[EMAIL]', clean_text)
            
            # ê¸°íƒ€ ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
            for pattern in self.unwanted_patterns[2:]:  # HTML ì—”í‹°í‹° ì œì™¸í•œ ë‚˜ë¨¸ì§€
                clean_text = pattern.sub('', clean_text)
            
            return clean_text
            
        except Exception as e:
            logger.warning(f"ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
            return text
    
    def extract_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
        if not text:
            return []
        
        try:
            # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„
            sentences = self.korean_sentence_pattern.split(text)
            
            # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
            clean_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 5:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                    clean_sentences.append(sentence)
            
            return clean_sentences
            
        except Exception as e:
            logger.warning(f"ë¬¸ì¥ ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return [text]
    
    def clean_job_posting_text(self, text: str) -> str:
        """ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ ì „ìš© ì •ì œ"""
        if not text:
            return ""
        
        logger.debug(f"ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ ì •ì œ ì‹œì‘: {len(text)} ë¬¸ì")
        
        # 1. HTML íƒœê·¸ ì œê±°
        clean_text = self.remove_html_tags(text)
        
        # 2. HTML ì—”í‹°í‹° ë””ì½”ë”©
        clean_text = self.decode_html_entities(clean_text)
        
        # 3. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        clean_text = self.normalize_unicode(clean_text)
        
        # 4. ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        clean_text = self.remove_unwanted_patterns(clean_text)
        
        # 5. ê³µë°± ì •ê·œí™”
        clean_text = self.normalize_whitespace(clean_text)
        
        # 6. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€)
        clean_text = self.remove_special_characters(clean_text, preserve_punctuation=True)
        
        logger.debug(f"ì±„ìš©ê³µê³  í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ: {len(clean_text)} ë¬¸ì")
        return clean_text
    
    def clean_cover_letter_text(self, text: str) -> str:
        """ìê¸°ì†Œê°œì„œ í…ìŠ¤íŠ¸ ì „ìš© ì •ì œ"""
        if not text:
            return ""
        
        logger.debug(f"ìê¸°ì†Œê°œì„œ í…ìŠ¤íŠ¸ ì •ì œ ì‹œì‘: {len(text)} ë¬¸ì")
        
        # 1. HTML íƒœê·¸ ì œê±°
        clean_text = self.remove_html_tags(text)
        
        # 2. HTML ì—”í‹°í‹° ë””ì½”ë”©
        clean_text = self.decode_html_entities(clean_text)
        
        # 3. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        clean_text = self.normalize_unicode(clean_text)
        
        # 4. ê³µë°± ì •ê·œí™”
        clean_text = self.normalize_whitespace(clean_text)
        
        # 5. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ìê¸°ì†Œê°œì„œëŠ” ë¬¸ì¥ë¶€í˜¸ ì¤‘ìš”í•˜ë¯€ë¡œ ìœ ì§€)
        clean_text = self.remove_special_characters(clean_text, preserve_punctuation=True)
        
        # 6. URLê³¼ ì´ë©”ì¼ì€ ì œê±°í•˜ì§€ ì•Šê³  ìœ ì§€ (ìê¸°ì†Œê°œì„œì—ì„œëŠ” ì˜ë¯¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        
        logger.debug(f"ìê¸°ì†Œê°œì„œ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ: {len(clean_text)} ë¬¸ì")
        return clean_text
    
    def validate_cleaned_text(self, original: str, cleaned: str, field_type: str = "general") -> Dict[str, any]:
        """ì •ì œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ ê²€ì¦"""
        if not original:
            return {"valid": False, "reason": "ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ"}
        
        try:
            original_length = len(original)
            cleaned_length = len(cleaned)
            
            # ê¸¸ì´ ë³€í™”ìœ¨ ê³„ì‚°
            length_ratio = cleaned_length / original_length if original_length > 0 else 0
            
            # í•œê¸€ ë¬¸ì ë¹„ìœ¨ í™•ì¸
            korean_chars = len(re.findall(r'[ê°€-í£]', cleaned))
            korean_ratio = korean_chars / cleaned_length if cleaned_length > 0 else 0
            
            # í•„ë“œ íƒ€ì…ë³„ ê²€ì¦ ê¸°ì¤€
            if field_type in ["title", "company", "position", "department"]:
                # ì§§ì€ í•„ë“œìš© ì™„í™”ëœ ê¸°ì¤€
                min_length = 2
                min_korean = 0.0
            elif field_type == "content":
                # ê¸´ ë‚´ìš©ìš© ê¸°ì¤€
                min_length = 20
                min_korean = 0.1
            else:
                # ì¼ë°˜ ê¸°ì¤€
                min_length = 3
                min_korean = 0.05
            
            validations = {
                "length_preserved": length_ratio >= 0.1,  # ì›ë³¸ì˜ 10% ì´ìƒ ìœ ì§€
                "has_content": cleaned_length >= min_length,
                "korean_content": korean_ratio >= min_korean,
                "no_html_tags": '<' not in cleaned and '>' not in cleaned,
                "proper_encoding": not any(char in cleaned for char in ['ï¿½', '\ufffd'])
            }
            
            is_valid = all(validations.values())
            
            return {
                "valid": is_valid,
                "original_length": original_length,
                "cleaned_length": cleaned_length,
                "length_ratio": round(length_ratio, 3),
                "korean_ratio": round(korean_ratio, 3),
                "validations": validations
            }
            
        except Exception as e:
            logger.warning(f"í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"valid": False, "reason": f"ê²€ì¦ ì˜¤ë¥˜: {e}"}

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    cleaner = TextCleaner()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "name": "HTML íƒœê·¸ê°€ í¬í•¨ëœ ì±„ìš©ê³µê³ ",
            "text": """
            <div class="job-description">
                <h2>ì£¼ìš”ì—…ë¬´</h2>
                <ul>
                    <li>ì‹œìŠ¤í…œ &amp; ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬</li>
                    <li>ë°ì´í„°ë² ì´ìŠ¤ ìš´ì˜&nbsp;&nbsp;ê´€ë¦¬</li>
                </ul>
                <p>ìê²©ìš”ê±´: <strong>10ë…„ ì´ìƒ</strong> ê²½ë ¥ì ìš°ëŒ€</p>
            </div>
            """,
            "type": "job_posting"
        },
        {
            "name": "íŠ¹ìˆ˜ë¬¸ìê°€ ë§ì€ ìê¸°ì†Œê°œì„œ",
            "text": """
            ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ!!! ì €ëŠ” ***ê¹€ì² ìˆ˜***ì…ë‹ˆë‹¤.
            
            [ê²½ë ¥ì‚¬í•­]
            - ITë¶„ì•¼ 15ë…„ ê²½ë ¥
            - í”„ë¡œì íŠ¸ ë§¤ë‹ˆì € ì—­í•  ìˆ˜í–‰
            
            ê°ì‚¬í•©ë‹ˆë‹¤. ^^
            """,
            "type": "cover_letter"
        }
    ]
    
    print("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}: {test_case['name']}")
        print("-" * 50)
        
        original_text = test_case['text']
        print(f"ì›ë³¸ í…ìŠ¤íŠ¸ (ê¸¸ì´: {len(original_text)}):")
        print(repr(original_text[:100]) + "..." if len(original_text) > 100 else repr(original_text))
        
        # ì •ì œ ì‹¤í–‰
        if test_case['type'] == 'job_posting':
            cleaned_text = cleaner.clean_job_posting_text(original_text)
        else:
            cleaned_text = cleaner.clean_cover_letter_text(original_text)
        
        print(f"\nì •ì œëœ í…ìŠ¤íŠ¸ (ê¸¸ì´: {len(cleaned_text)}):")
        print(repr(cleaned_text))
        
        # í’ˆì§ˆ ê²€ì¦
        validation = cleaner.validate_cleaned_text(original_text, cleaned_text)
        print(f"\ní’ˆì§ˆ ê²€ì¦: {'âœ… í†µê³¼' if validation['valid'] else 'âŒ ì‹¤íŒ¨'}")
        print(f"  - ê¸¸ì´ ë³´ì¡´ìœ¨: {validation.get('length_ratio', 0):.1%}")
        print(f"  - í•œê¸€ ë¹„ìœ¨: {validation.get('korean_ratio', 0):.1%}")

if __name__ == "__main__":
    main()