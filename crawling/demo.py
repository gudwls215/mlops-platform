"""
í¬ë¡¤ë§ ì‹œìŠ¤í…œ ë°ëª¨ (ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—†ì´)
"""
import logging
from scrapers.saramin_crawler import SaraminCrawler
from base_crawler import JobCrawlerUtils

def demo_crawling():
    """í¬ë¡¤ë§ ë°ëª¨ ì‹¤í–‰"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ MLOps í”Œë«í¼ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ë°ëª¨")
    print("="*50)
    
    try:
        # ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        print("ğŸ“‹ ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
        crawler = SaraminCrawler()
        
        # robots.txt í™•ì¸
        print("ğŸ¤– robots.txt ì¤€ìˆ˜ í™•ì¸...")
        test_url = "https://www.saramin.co.kr/zf_user/search/recruit"
        can_crawl = crawler.check_robots_txt(test_url)
        print(f"   í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€: {'âœ… í—ˆìš©' if can_crawl else 'âŒ ì°¨ë‹¨'}")
        
        # URL ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ í¬ë¡¤ë§ ì—†ì´)
        print("\nğŸ” ì±„ìš©ê³µê³  URL íŒ¨í„´ í…ŒìŠ¤íŠ¸...")
        print("   ëŒ€ìƒ í‚¤ì›Œë“œ:", ['ì‹œë‹ˆì–´', '50ëŒ€', 'ê²½ë ¥ì§'])
        print("   ì˜ˆìƒ ê²€ìƒ‰ URL íŒ¨í„´:")
        for keyword in ['ì‹œë‹ˆì–´', '50ëŒ€']:
            search_url = f"https://www.saramin.co.kr/zf_user/search/recruit?searchword={keyword}"
            print(f"   - {search_url}")
        
        # ì‹œë‹ˆì–´ ì¹œí™”ì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ‘´ ì‹œë‹ˆì–´ ì¹œí™”ì„± í•„í„°ë§ í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ì‹œë‹ˆì–´ ì¹œí™”ì 
        test_job1 = {
            'title': 'ì‹œë‹ˆì–´ í™˜ì˜ ê³ ê°ìƒë‹´ì›',
            'company': 'í…ŒìŠ¤íŠ¸íšŒì‚¬',
            'description': '50ëŒ€ ì´ìƒ ê²½í—˜ì ìš°ëŒ€',
            'category': 'ìƒë‹´',
            'requirements': 'ê²½ë ¥ ìš°ëŒ€'
        }
        
        is_senior_friendly1 = JobCrawlerUtils.is_senior_friendly(test_job1)
        print(f"   í…ŒìŠ¤íŠ¸ 1 - '{test_job1['title']}': {'âœ… ì í•©' if is_senior_friendly1 else 'âŒ ë¶€ì í•©'}")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ì‹œë‹ˆì–´ ë¶€ì í•©
        test_job2 = {
            'title': 'ì‹ ì… ê°œë°œì ëª¨ì§‘',
            'company': 'í…Œí¬íšŒì‚¬',
            'description': '20ëŒ€ 30ëŒ€ ì‹ ì… í™˜ì˜',
            'category': 'ê°œë°œ',
            'requirements': 'ì‹ ì…ë§Œ ì§€ì›'
        }
        
        is_senior_friendly2 = JobCrawlerUtils.is_senior_friendly(test_job2)
        print(f"   í…ŒìŠ¤íŠ¸ 2 - '{test_job2['title']}': {'âœ… ì í•©' if is_senior_friendly2 else 'âŒ ë¶€ì í•©'}")
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        print("\nâš™ï¸  í¬ë¡¤ë§ ì„¤ì • ì •ë³´:")
        from config import DELAY_MIN, DELAY_MAX, CONCURRENT_REQUESTS, SENIOR_KEYWORDS
        print(f"   ë”œë ˆì´ ë²”ìœ„: {DELAY_MIN}-{DELAY_MAX}ì´ˆ")
        print(f"   ë™ì‹œ ìš”ì²­ ì œí•œ: {CONCURRENT_REQUESTS}ê°œ")
        print(f"   ì‹œë‹ˆì–´ í‚¤ì›Œë“œ ìˆ˜: {len(SENIOR_KEYWORDS)}ê°œ")
        
        print("\nğŸ“Š í¬ë¡¤ë§ ì˜ˆìƒ ì„±ëŠ¥:")
        print(f"   í˜ì´ì§€ë‹¹ í‰ê·  ë”œë ˆì´: {(DELAY_MIN + DELAY_MAX) / 2}ì´ˆ")
        print(f"   ì‹œê°„ë‹¹ ì˜ˆìƒ ì²˜ë¦¬ëŸ‰: ~{3600 // ((DELAY_MIN + DELAY_MAX) / 2)}ê°œ í˜ì´ì§€")
        
        print("\nâœ… í¬ë¡¤ë§ ì¸í”„ë¼ êµ¬ì¶• ì™„ë£Œ!")
        print("   - ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ êµ¬í˜„ ì™„ë£Œ")
        print("   - ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬ êµ¬í˜„ ì™„ë£Œ") 
        print("   - ì‹œë‹ˆì–´ ì¹œí™”ì„± í•„í„°ë§ êµ¬í˜„ ì™„ë£Œ")
        print("   - robots.txt ì¤€ìˆ˜ ë¡œì§ êµ¬í˜„ ì™„ë£Œ")
        print("   - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ëª¨ë“ˆ êµ¬í˜„ ì™„ë£Œ")
        print("   - ìŠ¤ì¼€ì¤„ë§ ë° ìë™í™” ì§€ì› ì™„ë£Œ")
        
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        files = [
            "base_crawler.py - ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤",
            "config.py - ì„¤ì • íŒŒì¼", 
            "database.py - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™",
            "scrapers/saramin_crawler.py - ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬",
            "main.py - ë©”ì¸ ì‹¤í–‰ê¸°",
            "requirements.txt - ì˜ì¡´ì„± ëª©ë¡",
            "README.md - ì‚¬ìš©ë²• ê°€ì´ë“œ",
            ".env - í™˜ê²½ë³€ìˆ˜ ì„¤ì •"
        ]
        
        for file in files:
            print(f"   âœ“ {file}")
        
        print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ í™•ì¸ í›„ ì‹¤ì œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
        print("   2. ì¡ì½”ë¦¬ì•„, ì›Œí¬ë„· í¬ë¡¤ëŸ¬ ì¶”ê°€ ê°œë°œ")
        print("   3. í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ë° ìë™í™”")
        print("   4. API ì„œë²„ì™€ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì—°ë™")
        
    except Exception as e:
        print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    demo_crawling()