#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Linkareer ìê¸°ì†Œê°œì„œ í¬ë¡¤ëŸ¬
ì¥ë…„ì¸µì„ ìœ„í•œ ìê¸°ì†Œê°œì„œ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import time
import random
from urllib.parse import urljoin, parse_qs, urlparse
import json
import re
from datetime import datetime
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database import DatabaseManager

class LinkareerCoverLetterCrawler:
    def __init__(self):
        self.base_url = "https://linkareer.com"
        self.cover_letter_url = "https://linkareer.com/cover-letter"
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # ì‹œë‹ˆì–´ ì¹œí™”ì  í‚¤ì›Œë“œ
        self.senior_keywords = [
            'ê²½ë ¥', 'ì‹œë‹ˆì–´', 'ì±…ì„', 'ë§¤ë‹ˆì €', 'ë¶€ì¥', 'ê³¼ì¥', 'íŒ€ì¥', 
            'ë¦¬ë”', 'ê´€ë¦¬', 'ìš´ì˜', 'ê¸°íš', 'ì „ëµ', 'ì»¨ì„¤íŒ…', 'ë©˜í† ë§',
            '10ë…„', '15ë…„', '20ë…„', '25ë…„', '30ë…„', 'ê²½í—˜', 'ì „ë¬¸',
            'ë…¸í•˜ìš°', 'ì „ë¬¸ì„±', 'ìˆ™ë ¨', 'ë² í…Œë‘', 'ì‹¤ë¬´ì§„', 'í•µì‹¬ì¸ì¬'
        ]
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €
        self.db_manager = DatabaseManager()
        
    def get_cover_letter_list(self, page=1, max_pages=5):
        """ìê¸°ì†Œê°œì„œ ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§"""
        cover_letters = []
        
        for current_page in range(1, max_pages + 1):
            print(f"ğŸ“„ ìê¸°ì†Œê°œì„œ ëª©ë¡ í˜ì´ì§€ {current_page} í¬ë¡¤ë§ ì¤‘...")
            
            try:
                # í˜ì´ì§€ ìš”ì²­
                params = {'page': current_page}
                response = self.session.get(self.cover_letter_url, params=params, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ìê¸°ì†Œê°œì„œ ì•„ì´í…œ ì°¾ê¸°
                cover_letter_items = soup.find_all('div', class_='cover-letter-item') or \
                                   soup.find_all('article', class_='cover-letter') or \
                                   soup.find_all('div', class_='item') or \
                                   soup.select('.list-item, .card-item, .cover-letter-card')
                
                if not cover_letter_items:
                    # ë‹¤ë¥¸ ì…€ë ‰í„° ì‹œë„
                    cover_letter_items = soup.select('a[href*="cover-letter"]')
                
                print(f"  ì°¾ì€ ìê¸°ì†Œê°œì„œ ì•„ì´í…œ: {len(cover_letter_items)}ê°œ")
                
                if not cover_letter_items:
                    print(f"  í˜ì´ì§€ {current_page}ì—ì„œ ìê¸°ì†Œê°œì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # HTML êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œ ì¶œë ¥
                    print("  í˜ì´ì§€ HTML ìƒ˜í”Œ:")
                    print(response.text[:1000])
                    break
                
                # ê° ì•„ì´í…œ ì²˜ë¦¬
                for item in cover_letter_items:
                    try:
                        cover_letter_data = self.extract_cover_letter_preview(item)
                        if cover_letter_data and self.is_senior_friendly(cover_letter_data):
                            cover_letters.append(cover_letter_data)
                            print(f"  âœ… ì‹œë‹ˆì–´ ì¹œí™”ì  ìê¸°ì†Œê°œì„œ ë°œê²¬: {cover_letter_data.get('title', 'N/A')[:50]}...")
                        
                        time.sleep(random.uniform(0.5, 1.0))
                        
                    except Exception as e:
                        print(f"  âŒ ì•„ì´í…œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
                
                # í˜ì´ì§€ ê°„ ë”œë ˆì´
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {current_page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"ğŸ“Š ì´ {len(cover_letters)}ê°œì˜ ì‹œë‹ˆì–´ ì¹œí™”ì  ìê¸°ì†Œê°œì„œ ë°œê²¬")
        return cover_letters
    
    def extract_cover_letter_preview(self, item):
        """ìê¸°ì†Œê°œì„œ ë¯¸ë¦¬ë³´ê¸° ì •ë³´ ì¶”ì¶œ"""
        data = {}
        
        try:
            # URL ì¶”ì¶œ
            link = item.find('a') or item
            if link and link.get('href'):
                data['url'] = urljoin(self.base_url, link.get('href'))
            else:
                return None
            
            # ì œëª© ì¶”ì¶œ
            title_selectors = [
                '.title', '.subject', '.cover-letter-title', 'h3', 'h4', 
                '.item-title', '.card-title', '[class*="title"]'
            ]
            
            for selector in title_selectors:
                title_elem = item.select_one(selector)
                if title_elem:
                    data['title'] = title_elem.get_text().strip()
                    break
            
            if not data.get('title'):
                # ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                if link and link.get_text():
                    data['title'] = link.get_text().strip()
            
            # íšŒì‚¬ëª… ì¶”ì¶œ
            company_selectors = [
                '.company', '.company-name', '.corp', '[class*="company"]',
                '.info .company', '.meta .company'
            ]
            
            for selector in company_selectors:
                company_elem = item.select_one(selector)
                if company_elem:
                    data['company'] = company_elem.get_text().strip()
                    break
            
            # ì§ë¬´/ë¶€ì„œ ì¶”ì¶œ
            position_selectors = [
                '.position', '.job', '.dept', '.department', '[class*="position"]',
                '.job-title', '.role'
            ]
            
            for selector in position_selectors:
                position_elem = item.select_one(selector)
                if position_elem:
                    data['position'] = position_elem.get_text().strip()
                    break
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
            meta_info = item.select('.meta, .info, .details')
            for meta in meta_info:
                text = meta.get_text()
                
                # ì—°ë„ ì¶”ì¶œ
                year_match = re.search(r'20\d{2}', text)
                if year_match:
                    data['application_year'] = int(year_match.group())
                
                # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                views_match = re.search(r'ì¡°íšŒìˆ˜?\s*(\d+)', text)
                if views_match:
                    data['views'] = int(views_match.group(1))
                
                # ì¢‹ì•„ìš” ì¶”ì¶œ
                likes_match = re.search(r'ì¢‹ì•„ìš”\s*(\d+)', text)
                if likes_match:
                    data['likes'] = int(likes_match.group(1))
            
            # í•©ê²© ì—¬ë¶€ ì¶”ì¶œ
            pass_indicators = item.select('.pass, .success, [class*="pass"]')
            if pass_indicators:
                pass_text = ' '.join([elem.get_text() for elem in pass_indicators])
                data['is_passed'] = 'í•©ê²©' in pass_text or 'ìµœì¢…í•©ê²©' in pass_text
            
            return data
            
        except Exception as e:
            print(f"  âŒ ë¯¸ë¦¬ë³´ê¸° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def get_cover_letter_detail(self, url):
        """ìê¸°ì†Œê°œì„œ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            print(f"ğŸ“ ìƒì„¸ ìê¸°ì†Œê°œì„œ í¬ë¡¤ë§: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìê¸°ì†Œê°œì„œ ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                '.content', '.cover-letter-content', '.letter-content',
                '.main-content', '.body', '.description', '[class*="content"]'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for unwanted in content_elem.select('script, style, .ad, .advertisement'):
                        unwanted.decompose()
                    
                    content = content_elem.get_text().strip()
                    break
            
            if not content:
                # ì „ì²´ í˜ì´ì§€ì—ì„œ ìê¸°ì†Œê°œì„œ ë‚´ìš© ì¶”ì •
                all_text = soup.get_text()
                paragraphs = [p.strip() for p in all_text.split('\n') if len(p.strip()) > 50]
                if len(paragraphs) > 3:
                    content = '\n'.join(paragraphs[1:6])  # ìƒìœ„ ëª‡ ê°œ ë‹¨ë½ ì‚¬ìš©
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            meta_data = {}
            
            # ì œëª© ì¬ì¶”ì¶œ (ë” ì •í™•í•œ)
            title_selectors = ['.title', 'h1', 'h2', '.main-title', '.cover-letter-title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    meta_data['title'] = title_elem.get_text().strip()
                    break
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(content)
            meta_data['keywords'] = keywords
            
            return content, meta_data
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return None, None
    
    def extract_keywords(self, content):
        """ìê¸°ì†Œê°œì„œ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        keyword_patterns = [
            # ê¸°ìˆ  í‚¤ì›Œë“œ
            r'\b(Python|Java|JavaScript|React|SQL|AWS|Docker|Kubernetes|Git|Linux)\b',
            # ì—…ë¬´ í‚¤ì›Œë“œ  
            r'\b(ê¸°íš|ìš´ì˜|ê´€ë¦¬|ê°œë°œ|ì„¤ê³„|ë¶„ì„|ë§ˆì¼€íŒ…|ì˜ì—…|ì¸ì‚¬|ì¬ë¬´|íšŒê³„)\b',
            # ê²½ë ¥ í‚¤ì›Œë“œ
            r'\b(\d+ë…„|ê²½ë ¥|ê²½í—˜|ì „ë¬¸|ìˆ™ë ¨|ë² í…Œë‘|ì‹œë‹ˆì–´)\b',
            # ì„±ê³¼ í‚¤ì›Œë“œ
            r'\b(ì„±ê³¼|ë‹¬ì„±|ê°œì„ |íš¨ìœ¨|ì ˆì•½|ì¦ê°€|í–¥ìƒ|ìµœì í™”)\b'
        ]
        
        for pattern in keyword_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        keywords = list(set([kw.strip() for kw in keywords if len(kw.strip()) > 1]))
        return keywords[:10]  # ìƒìœ„ 10ê°œë§Œ
    
    def is_senior_friendly(self, cover_letter_data):
        """ì‹œë‹ˆì–´ ì¹œí™”ì  ìê¸°ì†Œê°œì„œì¸ì§€ íŒë‹¨"""
        text_to_check = f"{cover_letter_data.get('title', '')} {cover_letter_data.get('company', '')} {cover_letter_data.get('position', '')}"
        
        # ì‹œë‹ˆì–´ í‚¤ì›Œë“œ í™•ì¸
        for keyword in self.senior_keywords:
            if keyword in text_to_check:
                return True
        
        # ì—°ë„ ê¸°ì¤€ (ìµœê·¼ 5ë…„ ì´ë‚´)
        current_year = datetime.now().year
        app_year = cover_letter_data.get('application_year')
        if app_year and current_year - app_year <= 5:
            return True
        
        # ì¡°íšŒìˆ˜ ê¸°ì¤€ (ì¸ê¸° ìˆëŠ” ìê¸°ì†Œê°œì„œ)
        views = cover_letter_data.get('views', 0)
        if views > 100:
            return True
        
        return False
    
    def save_cover_letter(self, cover_letter_data, content):
        """ìê¸°ì†Œê°œì„œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            # ë°ì´í„° ì¤€ë¹„
            data = {
                'title': cover_letter_data.get('title', ''),
                'company': cover_letter_data.get('company', ''),
                'position': cover_letter_data.get('position'),
                'department': cover_letter_data.get('department'),
                'experience_level': cover_letter_data.get('experience_level'),
                'content': content or '',
                'is_passed': cover_letter_data.get('is_passed'),
                'application_year': cover_letter_data.get('application_year'),
                'keywords': cover_letter_data.get('keywords', []),
                'url': cover_letter_data.get('url'),
                'views': cover_letter_data.get('views', 0),
                'likes': cover_letter_data.get('likes', 0)
            }
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            success = self.db_manager.insert_cover_letter_sample(data)
            
            if success:
                print(f"âœ… ìê¸°ì†Œê°œì„œ ì €ì¥ ì„±ê³µ: {data['title'][:50]}...")
                return True
            else:
                print(f"âŒ ìê¸°ì†Œê°œì„œ ì €ì¥ ì‹¤íŒ¨: {data['title'][:50]}...")
                return False
                
        except Exception as e:
            print(f"âŒ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def crawl_cover_letters(self, max_pages=3, max_details=20):
        """ì „ì²´ ìê¸°ì†Œê°œì„œ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ Linkareer ìê¸°ì†Œê°œì„œ í¬ë¡¤ë§ ì‹œì‘")
        print("="*60)
        
        try:
            # 1. ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§
            cover_letter_list = self.get_cover_letter_list(max_pages=max_pages)
            
            if not cover_letter_list:
                print("âŒ ìê¸°ì†Œê°œì„œ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ë° ì €ì¥
            saved_count = 0
            
            for i, cover_letter_data in enumerate(cover_letter_list[:max_details]):
                print(f"\nğŸ“ [{i+1}/{min(len(cover_letter_list), max_details)}] ìƒì„¸ í¬ë¡¤ë§ ì¤‘...")
                
                if not cover_letter_data.get('url'):
                    print("  URLì´ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                    continue
                
                # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
                content, meta_data = self.get_cover_letter_detail(cover_letter_data['url'])
                
                if content:
                    # ë©”íƒ€ë°ì´í„° ë³‘í•©
                    if meta_data:
                        cover_letter_data.update(meta_data)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    if self.save_cover_letter(cover_letter_data, content):
                        saved_count += 1
                
                # ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(random.uniform(2, 4))
            
            print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ë°œê²¬: {len(cover_letter_list)}ê°œ")
            print(f"ğŸ’¾ ì €ì¥ ì„±ê³µ: {saved_count}ê°œ")
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = LinkareerCoverLetterCrawler()
    
    # í¬ë¡¤ë§ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì†ŒëŸ‰)
    crawler.crawl_cover_letters(max_pages=2, max_details=5)

if __name__ == "__main__":
    main()