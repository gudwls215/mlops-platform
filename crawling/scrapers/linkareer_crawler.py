#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Linkareer ìê¸°ì†Œê°œì„œ í¬ë¡¤ëŸ¬
ì¥ë…„ì¸µì„ ìœ„í•œ ìê¸°ì†Œê°œì„œ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import random
import re
from datetime import datetime
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

from database import DatabaseManager

class LinkareerCoverLetterCrawler:
    def __init__(self):
        self.base_url = "https://linkareer.com"
        self.cover_letter_url = "https://linkareer.com/cover-letter/search"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €
        self.db_manager = DatabaseManager()
        
        

    # --- 2ë‹¨ê³„: ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜  ---
    async def scrape_detail_page(self, page, url):
        """
        ê°œë³„ ìì†Œì„œ URLì— ì ‘ì†í•˜ì—¬ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        
        ì´ í•¨ìˆ˜ê°€ í•˜ëŠ” ì¼:
        1. ì£¼ì–´ì§„ URL(ìì†Œì„œ í˜ì´ì§€)ì— ì ‘ì†
        2. HTMLì—ì„œ íšŒì‚¬ëª…, ì§ë¬´, í•©ê²©ìŠ¤í™, ìì†Œì„œ ë‚´ìš©ì„ ì°¾ì•„ì„œ ê°€ì ¸ì˜´
        3. ë°ì´í„°ë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì„œ ë°˜í™˜
        
        ë§¤ê°œë³€ìˆ˜:
            page: ì›¹ ë¸Œë¼ìš°ì € í˜ì´ì§€ ê°ì²´
            url: ë°©ë¬¸í•  ìì†Œì„œ í˜ì´ì§€ ì£¼ì†Œ
        
        ë°˜í™˜ê°’:
            ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ìì†Œì„œ ì •ë³´ (íšŒì‚¬, ì§ë¬´, ìŠ¤í™, ìì†Œì„œ ë‚´ìš© ë“±)
        """
        print(f"  > ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì¤‘... {url}")
        try:
            # ì›¹í˜ì´ì§€ ì ‘ì† (ìµœëŒ€ 20ì´ˆ ëŒ€ê¸°)
            await page.goto(url, wait_until='domcontentloaded', timeout=20000)
            
            # HTMLì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (íšŒì‚¬/ì§ë¬´/ì‹œê¸°)
            basic_info_text = await page.locator('h1.basic-info').first.inner_text()
            parts = [p.strip() for p in basic_info_text.split('/')]  # '/'ë¡œ ë‚˜ëˆ„ì–´ì„œ ì •ë¦¬
            company = parts[0] if len(parts) > 0 else ''  # íšŒì‚¬ëª…
            role = parts[1] if len(parts) > 1 else ''     # ì§ë¬´
            period = parts[2] if len(parts) > 2 else ''   # ì‹œê¸°

            # í•©ê²©ìŠ¤í™ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í•™ì , ì–´í•™ì ìˆ˜ ë“±)
            spec_info_text = await page.locator('h3.spec-info').first.inner_text()
            
            # ìê¸°ì†Œê°œì„œ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            cover_letter_content = await page.locator('main.dwBPHz').first.inner_text()
            
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì»´í“¨í„°ê°€ ì½ê¸° ì–´ë ¤ìš´ ë¬¸ìë“¤)
            clean_cover_letter = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cover_letter_content)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
            cover_letter_data = {
                'title': f"{company} {role} ({period})",
                'company': company,
                'position': role,
                'application_period': period,
                'spec': spec_info_text,
                'content': clean_cover_letter,
                'url': url,
                'application_year': self.extract_year_from_period(period)
            }
            
            return cover_letter_data
            
        except Exception as e:
            # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥í•˜ê³  None ë°˜í™˜
            print(f"  [ì˜¤ë¥˜] ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {url}, ì›ì¸: {e}")
            return None
    
    def extract_year_from_period(self, period):
        """ì‹œê¸° ë¬¸ìì—´ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        try:
            year_match = re.search(r'20\d{2}', period)
            if year_match:
                return int(year_match.group())
            return datetime.now().year
        except:
            return datetime.now().year

    # --- 1ë‹¨ê³„: ë§í¬ ìˆ˜ì§‘ í•¨ìˆ˜ ---
    async def get_all_data_and_save_to_excel(self, start_page, end_page):
        """
        ë§ì»¤ë¦¬ì–´ì—ì„œ ìì†Œì„œ ë§í¬ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì¸ í•¨ìˆ˜
        
        ì´ í•¨ìˆ˜ê°€ í•˜ëŠ” ì¼:
        1. ì§€ì •ëœ í˜ì´ì§€ ë²”ìœ„ì—ì„œ ìì†Œì„œ ë§í¬ë“¤ì„ ëª¨ìŒ
        2. ê° ë§í¬ì— ë“¤ì–´ê°€ì„œ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘
        3. ëª¨ë“  ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì„œ ë°˜í™˜
        
        ë§¤ê°œë³€ìˆ˜:
            start_page: ì‹œì‘í•  í˜ì´ì§€ ë²ˆí˜¸ (ì˜ˆ: 1)
            end_page: ëë‚¼ í˜ì´ì§€ ë²ˆí˜¸ (ì˜ˆ: 10)
        
        ë°˜í™˜ê°’:
            ëª¨ë“  ìì†Œì„œ ë°ì´í„°ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
        """
        all_links = []        # ìˆ˜ì§‘í•œ ëª¨ë“  ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        detailed_data = []    # ìƒì„¸ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

        # Playwrightë¡œ ì›¹ ë¸Œë¼ìš°ì € ì‹¤í–‰
        async with async_playwright() as p:
            # í¬ë¡¬ ë¸Œë¼ìš°ì €ë¥¼ headless ëª¨ë“œë¡œ ì‹¤í–‰ (í™”ë©´ì— ì•ˆ ë³´ì´ê²Œ)
            browser = await p.chromium.launch(headless=True)
            
            # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì • (ë´‡ì´ ì•„ë‹Œ ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ)
            context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            page = await context.new_page()

            # ì§€ì •ëœ í˜ì´ì§€ ë²”ìœ„ë§Œí¼ ë°˜ë³µ
            for i in range(start_page, end_page + 1):
                print(f"\n[INFO] {i}í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ ì‹œì‘...")
                try: 
                    # ë§ì»¤ë¦¬ì–´ ìì†Œì„œ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                    target_url = f"https://linkareer.com/cover-letter/search?page={i}&tab=all"
                    await page.goto(target_url, wait_until='networkidle', timeout=30000)
                    await page.wait_for_timeout(1000) # í˜ì´ì§€ ë¡œë“œ í›„ 1ì´ˆ ëŒ€ê¸° (ì•ˆì •ì„±ì„ ìœ„í•´)
                    print(f"[SUCCESS] {i}í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    # í˜ì´ì§€ì—ì„œ ëª¨ë“  ìì†Œì„œ ë§í¬ë¥¼ ì°¾ì•„ì„œ ê°€ì ¸ì˜´
                    found_links = await page.eval_on_selector_all(
                        'a.link',  # CSS ì„ íƒì: classê°€ 'link'ì¸ ëª¨ë“  <a> íƒœê·¸
                        'elements => elements.map(el => el.getAttribute("href"))'  # href ì†ì„±ê°’ ì¶”ì¶œ
                    )
                    
                    # ê° ë§í¬ë¥¼ í™•ì¸í•˜ê³  ì¤‘ë³µë˜ì§€ ì•Šìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    for href in found_links:
                        # ìƒëŒ€ê²½ë¡œë©´ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
                        link = "https://linkareer.com" + href if href.startswith('/') else href
                        # ìì†Œì„œ ë§í¬ì´ê³  ì•„ì§ ì—†ëŠ” ë§í¬ë©´ ì¶”ê°€
                        if '/cover-letter/' in link and link not in all_links:
                            all_links.append(link)
                
                # í˜ì´ì§€ ë¡œë”© ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´
                except PlaywrightTimeoutError:
                    print(f"[ERROR] {i}í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼. ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                    continue  # ì´ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê³  ë‹¤ìŒ í˜ì´ì§€ë¡œ
                
                # ê·¸ ì™¸ ë‹¤ë¥¸ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´
                except Exception as e:
                    print(f"[ERROR] {i}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    print("ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆê±°ë‚˜ í˜ì´ì§€ ì´ë™ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ì—¬ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break  # ë°˜ë³µë¬¸ ì¢…ë£Œ
            
            print(f"\n--- [1ë‹¨ê³„ ì™„ë£Œ] ì´ {len(all_links)}ê°œì˜ ê³ ìœ  ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ ---")
            
            print("\n--- [2ë‹¨ê³„ ì‹œì‘] ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
            # ìˆ˜ì§‘í•œ ëª¨ë“  ë§í¬ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            saved_count = 0
            for i, link in enumerate(all_links):
                print(f"[{i+1}/{len(all_links)}] ì²˜ë¦¬ ì¤‘: {link}")
                data = await self.scrape_detail_page(page, link)
                if data:  # ì •ìƒì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìœ¼ë©´
                    detailed_data.append(data)
                    # ë°”ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    if self.save_cover_letter(data, data.get('content', '')):
                        saved_count += 1
                        print(f"    âœ… ì €ì¥ ì„±ê³µ: {data.get('title', 'Unknown')[:50]}...")
                    else:
                        print(f"    âŒ ì €ì¥ ì‹¤íŒ¨: {data.get('title', 'Unknown')[:50]}...")
                
                # ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€) 
                await page.wait_for_timeout(random.randint(2000, 4000))
            
            await browser.close()  # ë¸Œë¼ìš°ì € ì¢…ë£Œ
            
            print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ë°œê²¬: {len(all_links)}ê°œ")
            print(f"ğŸ’¾ ì €ì¥ ì„±ê³µ: {saved_count}ê°œ")
            
        return detailed_data


    def save_cover_letter(self, cover_letter_data, content):
        """ìê¸°ì†Œê°œì„œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            # ë°ì´í„° ì¤€ë¹„
            data = {
                'title': cover_letter_data.get('title', ''),
                'company': cover_letter_data.get('company', ''),
                'position': cover_letter_data.get('position'),
                'department': cover_letter_data.get('department'),
                'experience_level': cover_letter_data.get('experience_level'),
                'content': content or cover_letter_data.get('content', ''),
                'is_passed': cover_letter_data.get('is_passed'),
                'application_year': cover_letter_data.get('application_year'),
                'application_period': cover_letter_data.get('application_period'),
                'company_type': cover_letter_data.get('company_type'),
                'spec': cover_letter_data.get('spec'),
                'keywords': cover_letter_data.get('keywords', []),
                'url': cover_letter_data.get('url'),
                'scrap_count': cover_letter_data.get('scrap_count', 0),
                'views': cover_letter_data.get('views', 0),
                'likes': cover_letter_data.get('likes', 0)
            }
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            success = self.db_manager.insert_cover_letter_sample(data)
            
            if success:
                print(f"âœ… ìê¸°ì†Œê°œì„œ ì €ì¥ ì„±ê³µ: {data['title'][:50]}...")
                return True
            else:
                print(f"âŒ ìê¸°ì†Œê°œì„œ ì €ì¥ ì‹¤íŒ¨: {data['title'][:50]}...")
                return False
                
        except Exception as e:
            print(f"âŒ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    


    async def crawl_cover_letters_playwright(self, max_pages=3):
        """Playwrightë¥¼ ì‚¬ìš©í•œ ì „ì²´ ìê¸°ì†Œê°œì„œ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ Linkareer ìê¸°ì†Œê°œì„œ í¬ë¡¤ë§ ì‹œì‘ (Playwright ë°©ì‹)")
        print("="*60)
        
        try:
            # get_all_data_and_save_to_excelì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§
            detailed_data = await self.get_all_data_and_save_to_excel(1, max_pages)
            
            print(f"\nğŸ‰ Playwright í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(detailed_data)}ê°œ")
            
            return detailed_data
            
        except Exception as e:
            print(f"âŒ Playwright í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("âš ï¸ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            return []

    def crawl(self, max_items=50):
        """DAGì—ì„œ í˜¸ì¶œí•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œ"""
        try:
            print(f"ğŸš€ Linkareer í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_items}ê°œ, Playwright ë°©ì‹)")
            
            # max_itemsë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
            max_pages = max(1, max_items // 10)  # í˜ì´ì§€ë‹¹ ì•½ 10ê°œ ê°€ì •
            
            # asyncioë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰
            detailed_data = asyncio.run(self.crawl_cover_letters_playwright(max_pages=max_pages))
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "status": "completed", 
                "message": f"í¬ë¡¤ë§ ì™„ë£Œ - {len(detailed_data)}ê°œ ìˆ˜ì§‘",
                "data_count": len(detailed_data)
            }
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return {"status": "failed", "message": str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Linkareer í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*50)
    
    try:
        crawler = LinkareerCoverLetterCrawler()
        
        # í¬ë¡¤ë§ ì‹¤í–‰ (Playwright ë°©ì‹)
        print("\nğŸ“‹ í¬ë¡¤ë§ ì‹œì‘...")
        result = crawler.crawl(max_items=20)
        print(f"ê²°ê³¼: {result}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        print("âš ï¸ í¬ë¡¤ë§ì„ ì¬ì‹œë„í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    print("\nğŸ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")

if __name__ == "__main__":
    main()