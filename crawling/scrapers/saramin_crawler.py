"""
ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬
"""
import re
import urllib.parse
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆë“¤ì„ importí•  ìˆ˜ ìˆë„ë¡ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from base_crawler import RequestsCrawler, JobCrawlerUtils
from config import TARGET_SITES, SENIOR_KEYWORDS


class SaraminCrawler(RequestsCrawler):
    """ì‚¬ëŒì¸ ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        site_config = TARGET_SITES['saramin']
        super().__init__('saramin', site_config['base_url'], site_config['robots_url'])
        self.job_search_url = site_config['job_search_url']
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¶”ê°€
        from database import DatabaseManager
        self.db_manager = DatabaseManager()
    
    def get_job_urls(self, category: str = None, page_limit: int = 5) -> List[str]:
        """ì±„ìš©ê³µê³  URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        job_urls = []
        
        # ì‹œë‹ˆì–´ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        for keyword in SENIOR_KEYWORDS[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            try:
                for page in range(1, page_limit + 1):
                    search_url = f"{self.base_url}/zf_user/search/recruit?searchType=search&searchword={urllib.parse.quote(keyword)}&recruitPage={page}"
                    
                    self.logger.info(f"ê²€ìƒ‰ í˜ì´ì§€ í¬ë¡¤ë§: {search_url}")
                    html = self.fetch_page(search_url)
                    
                    if not html:
                        continue
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ì±„ìš©ê³µê³  ë§í¬ ì¶”ì¶œ (ì‹¤ì œ ì‚¬ëŒì¸ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                    job_links = soup.select('.item_recruit a[href*="/zf_user/jobs/relay/view"]')
                    
                    if not job_links:
                        self.logger.warning(f"ì±„ìš©ê³µê³  ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {search_url}")
                        break
                    
                    for link in job_links:
                        href = link.get('href')
                        if href:
                            if href.startswith('/'):
                                full_url = self.base_url + href
                            else:
                                full_url = href
                            
                            if full_url not in job_urls:
                                job_urls.append(full_url)
                    
                    self.logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {len(job_links)}ê°œ ë§í¬ ìˆ˜ì§‘")
                    
            except Exception as e:
                self.logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì´ {len(job_urls)}ê°œ ì±„ìš©ê³µê³  URL ìˆ˜ì§‘ ì™„ë£Œ")
        return job_urls
    
    def parse_job_listing(self, html: str, url: str = '') -> Optional[Dict]:
        """ê°œë³„ ì±„ìš©ê³µê³  íŒŒì‹±"""
        if not html:
            return None

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            job_data = {
                'source': 'saramin',  # site -> sourceë¡œ ë³€ê²½
                'url': url,
                'title': '',
                'company': '',
                'location': '',
                'salary': '',
                'employment_type': '',
                'experience': '',
                'education': '',
                'category': '',
                'description': '',
                'requirements': '',
                'benefits': '',
                'deadline': '',
                'posted_date': '',
                'tags': [],
                'main_duties': '',
                'qualifications': '',
                'preferences': ''
            }            # ì œëª© (í˜ì´ì§€ title íƒœê·¸ì—ì„œ ì¶”ì¶œ)
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text()
                # "íšŒì‚¬ëª…] ì±„ìš©ê³µê³ ì œëª© - ì‚¬ëŒì¸" í˜•íƒœì—ì„œ ì œëª© ë¶€ë¶„ ì¶”ì¶œ
                if '] ' in title_text and ' - ì‚¬ëŒì¸' in title_text:
                    title_part = title_text.split('] ', 1)[1].split(' - ì‚¬ëŒì¸')[0]
                    job_data['title'] = JobCrawlerUtils.clean_text(title_part)
            
            # íšŒì‚¬ëª… (í˜ì´ì§€ title íƒœê·¸ì—ì„œ ì¶”ì¶œ)
            if title_tag:
                title_text = title_tag.get_text()
                # "[íšŒì‚¬ëª…] ì±„ìš©ê³µê³ ì œëª© - ì‚¬ëŒì¸" í˜•íƒœì—ì„œ íšŒì‚¬ëª… ë¶€ë¶„ ì¶”ì¶œ
                if title_text.startswith('[') and '] ' in title_text:
                    company_part = title_text.split('[')[1].split(']')[0]
                    job_data['company'] = JobCrawlerUtils.clean_text(company_part)
            
            # ê·¼ë¬´ì§€ì—­
            location_elem = soup.find('div', class_='recruit_condition')
            if location_elem:
                location_text = location_elem.get_text()
                # ê·¼ë¬´ì§€ì—­ ì¶”ì¶œ
                location_match = re.search(r'ê·¼ë¬´ì§€ì—­\s*([^Â·]+)', location_text)
                if location_match:
                    job_data['location'] = JobCrawlerUtils.clean_text(location_match.group(1))
            
            # ê¸‰ì—¬
            salary_elem = soup.find('div', class_='recruit_condition')
            if salary_elem:
                salary_text = salary_elem.get_text()
                salary = JobCrawlerUtils.extract_salary(salary_text)
                if salary:
                    job_data['salary'] = salary
            
            # ê³ ìš©í˜•íƒœ
            employment_elem = soup.find('div', class_='recruit_condition')
            if employment_elem:
                employment_text = employment_elem.get_text()
                if 'ì •ê·œì§' in employment_text:
                    job_data['employment_type'] = 'ì •ê·œì§'
                elif 'ê³„ì•½ì§' in employment_text:
                    job_data['employment_type'] = 'ê³„ì•½ì§'
                elif 'íŒŒíŠ¸íƒ€ì„' in employment_text or 'ì‹œê°„ì œ' in employment_text:
                    job_data['employment_type'] = 'íŒŒíŠ¸íƒ€ì„'
            
            # ê²½ë ¥ ìš”êµ¬ì‚¬í•­
            experience_elem = soup.find('div', class_='recruit_condition')
            if experience_elem:
                exp_text = experience_elem.get_text()
                if 'ê²½ë ¥ë¬´ê´€' in exp_text:
                    job_data['experience'] = 'ê²½ë ¥ë¬´ê´€'
                elif 'ì‹ ì…' in exp_text:
                    job_data['experience'] = 'ì‹ ì…'
                else:
                    exp_match = re.search(r'(\d+)ë…„\s*ì´ìƒ', exp_text)
                    if exp_match:
                        job_data['experience'] = f"{exp_match.group(1)}ë…„ ì´ìƒ"
            
            # í•™ë ¥ ìš”êµ¬ì‚¬í•­
            education_elem = soup.find('div', class_='recruit_condition')
            if education_elem:
                edu_text = education_elem.get_text()
                if 'í•™ë ¥ë¬´ê´€' in edu_text:
                    job_data['education'] = 'í•™ë ¥ë¬´ê´€'
                elif 'ê³ ë“±í•™êµ' in edu_text:
                    job_data['education'] = 'ê³ ë“±í•™êµ'
                elif 'ëŒ€í•™êµ' in edu_text:
                    job_data['education'] = 'ëŒ€í•™êµ'
            
            # ì§ë¬´ ì¹´í…Œê³ ë¦¬
            category_elem = soup.find('div', class_='recruit_condition')
            if category_elem:
                category_text = category_elem.get_text()
                # ì§ë¬´ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ë¡œì§
                job_data['category'] = JobCrawlerUtils.clean_text(category_text[:50])
            
            # dt, dd ìŒì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            dt_elements = soup.find_all('dt')
            for dt in dt_elements:
                dt_text = dt.get_text().strip()
                dd = dt.find_next_sibling('dd')
                if dd:
                    dd_text = JobCrawlerUtils.clean_text(dd.get_text())
                    
                    # ê° í•„ë“œë³„ë¡œ ë§¤í•‘
                    if 'ê²½ë ¥' in dt_text:
                        job_data['experience'] = dd_text
                    elif 'í•™ë ¥' in dt_text:
                        job_data['education'] = dd_text
                    elif 'ê·¼ë¬´í˜•íƒœ' in dt_text or 'ê³ ìš©í˜•íƒœ' in dt_text:
                        job_data['employment_type'] = dd_text
                    elif 'ê¸‰ì—¬' in dt_text or 'ì—°ë´‰' in dt_text:
                        job_data['salary'] = dd_text
                    elif 'ê·¼ë¬´ì§€ì—­' in dt_text or 'ê·¼ë¬´ì§€' in dt_text:
                        job_data['location'] = dd_text
                    elif 'ë§ˆê°ì¼' in dt_text:
                        job_data['deadline'] = dd_text
                    elif 'ì‹œì‘ì¼' in dt_text or 'ë“±ë¡ì¼' in dt_text:
                        job_data['posted_date'] = dd_text
                    elif 'ìê²©ìš”ê±´' in dt_text:
                        job_data['qualifications'] = dd_text
                    elif 'ìš°ëŒ€ì‚¬í•­' in dt_text:
                        job_data['preferences'] = dd_text

            # ì£¼ìš”ì—…ë¬´, ìê²©ìš”ê±´, ìš°ëŒ€ì‚¬í•­ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ ì¶”ì¶œ
            page_text = soup.get_text()
            
            # ì£¼ìš”ì—…ë¬´ ì¶”ì¶œ
            if 'ì£¼ìš”ì—…ë¬´' in page_text:
                main_duties_elements = soup.find_all(string=re.compile('ì£¼ìš”ì—…ë¬´'))
                for element in main_duties_elements:
                    parent = element.parent
                    if parent:
                        siblings = parent.find_next_siblings()
                        for sibling in siblings[:2]:
                            sibling_text = JobCrawlerUtils.clean_text(sibling.get_text())
                            if sibling_text and len(sibling_text) > 10:
                                job_data['main_duties'] = sibling_text
                                break

            # ê·¼ë¬´ì¡°ê±´ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            if 'ê·¼ë¬´ì¡°ê±´' in page_text:
                condition_match = re.search(r'ê·¼ë¬´ì¡°ê±´.*?(?=\n|$)', page_text, re.DOTALL)
                if condition_match:
                    condition_text = condition_match.group(0)
                    # ê³ ìš©í˜•íƒœ ì¶”ì¶œ
                    if 'ì •ê·œì§' in condition_text:
                        job_data['employment_type'] = 'ì •ê·œì§'
                    elif 'ê³„ì•½ì§' in condition_text:
                        job_data['employment_type'] = 'ê³„ì•½ì§'
                    
                    # ê¸‰ì—¬ ì •ë³´ ì¶”ì¶œ
                    salary_match = re.search(r'ê¸‰ì—¬[:\s]*([^â€¢\n]+)', condition_text)
                    if salary_match:
                        job_data['salary'] = JobCrawlerUtils.clean_text(salary_match.group(1))
                    
                    # ê·¼ë¬´ì§€ ì¶”ì¶œ
                    location_match = re.search(r'ê·¼ë¬´ì§€[:\s]*([^â€¢\n]+)', condition_text)
                    if location_match:
                        job_data['location'] = JobCrawlerUtils.clean_text(location_match.group(1))

            # ì „ì²´ ì„¤ëª…ì€ í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì±„ìš©ê³µê³  ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if not job_data['description']:
                # ì œëª© ì´í›„ í…ìŠ¤íŠ¸ë¥¼ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
                if job_data['title'] in page_text:
                    title_idx = page_text.find(job_data['title'])
                    desc_text = page_text[title_idx + len(job_data['title']):title_idx + 500]
                    job_data['description'] = JobCrawlerUtils.clean_text(desc_text)            # ë§ˆê°ì¼
            deadline_elem = soup.find('div', class_='recruit_date')
            if deadline_elem:
                deadline_text = deadline_elem.get_text()
                deadline_match = re.search(r'(\d{4}-\d{2}-\d{2})', deadline_text)
                if deadline_match:
                    job_data['deadline'] = deadline_match.group(1)
            
            # ë“±ë¡ì¼
            posted_elem = soup.find('div', class_='recruit_date')
            if posted_elem:
                posted_text = posted_elem.get_text()
                posted_match = re.search(r'ë“±ë¡ì¼\s*(\d{4}-\d{2}-\d{2})', posted_text)
                if posted_match:
                    job_data['posted_date'] = posted_match.group(1)
            
            # íƒœê·¸
            tag_elems = soup.find_all('span', class_='tag')
            job_data['tags'] = [JobCrawlerUtils.clean_text(tag.get_text()) for tag in tag_elems]
            
            # ì‹œë‹ˆì–´ ì¹œí™”ì ì¸ì§€ í™•ì¸
            is_senior_friendly = JobCrawlerUtils.is_senior_friendly(job_data)
            if not is_senior_friendly:
                self.logger.warning(f"ì‹œë‹ˆì–´ ì¹œí™”ì ì´ì§€ ì•Šì€ ê³µê³ : {job_data['title']} - {job_data['company']}")
                return None
            else:
                self.logger.info(f"ì‹œë‹ˆì–´ ì¹œí™”ì  ê³µê³  ë°œê²¬: {job_data['title']} - {job_data['company']}")
            
            self.logger.info(f"ì±„ìš©ê³µê³  íŒŒì‹± ì™„ë£Œ: {job_data['title']} - {job_data['company']}")
            return job_data
            
        except Exception as e:
            self.logger.error(f"ì±„ìš©ê³µê³  íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def crawl_jobs(self, max_jobs: int = 100, save_to_db: bool = True) -> List[Dict]:
        """ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì‹¤í–‰"""
        self.logger.info(f"ì‚¬ëŒì¸ ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_jobs}ê°œ, DB ì €ì¥: {save_to_db})")
        
        # URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        job_urls = self.get_job_urls()
        
        if not job_urls:
            self.logger.warning("í¬ë¡¤ë§í•  ì±„ìš©ê³µê³  URLì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        job_urls = job_urls[:max_jobs]
        
        crawled_jobs = []
        saved_count = 0
        
        for i, url in enumerate(job_urls, 1):
            try:
                self.logger.info(f"ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì¤‘ ({i}/{len(job_urls)}): {url}")
                
                # ì‹¤ì œ ì±„ìš©ê³µê³  ìƒì„¸ í˜ì´ì§€ URLë¡œ ë³€ê²½
                detail_url = url.replace('/zf_user/jobs/relay/view', '/zf_user/jobs/view')
                detail_url = re.sub(r'&[^=]+=([^&]*)', '', detail_url)  # ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
                if '?' not in detail_url:
                    detail_url += '?'
                if 'rec_idx=' not in detail_url:
                    rec_idx_match = re.search(r'rec_idx=(\d+)', url)
                    if rec_idx_match:
                        detail_url = f"https://www.saramin.co.kr/zf_user/jobs/view?rec_idx={rec_idx_match.group(1)}"
                
                html = self.fetch_page(detail_url)
                if not html:
                    continue
                
                job_data = self.parse_job_listing(html, url)
                if job_data:
                    crawled_jobs.append(job_data)
                    
                    # ì‹¤ì‹œê°„ DB ì €ì¥ (Linkareer ë°©ì‹ê³¼ ë™ì¼)
                    if save_to_db:
                        try:
                            job_id = self.db_manager.insert_job_posting(job_data)
                            if job_id:
                                saved_count += 1
                                self.logger.info(f"âœ… ì±„ìš©ê³µê³  ì €ì¥ ì„±ê³µ - ID: {job_id}, ì œëª©: {job_data['title']}")
                            else:
                                self.logger.warning(f"âŒ ì±„ìš©ê³µê³  ì €ì¥ ì‹¤íŒ¨: {job_data['title']}")
                        except Exception as save_error:
                            self.logger.error(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {save_error}")
                    else:
                        self.logger.info(f"ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì„±ê³µ: {job_data['title']}")
                
            except Exception as e:
                self.logger.error(f"ì±„ìš©ê³µê³  í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
                continue
        
        if save_to_db:
            self.logger.info(f"ğŸ‰ ì‚¬ëŒì¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(crawled_jobs)}ê°œ ìˆ˜ì§‘, {saved_count}ê°œ DB ì €ì¥")
        else:
            self.logger.info(f"ì‚¬ëŒì¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(crawled_jobs)}ê°œ ì±„ìš©ê³µê³  ìˆ˜ì§‘ (DB ì €ì¥ ì—†ìŒ)")
        
        return crawled_jobs


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    crawler = SaraminCrawler()
    
    # í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§
    jobs = crawler.crawl_jobs(max_jobs=10)
    
    print(f"\n=== í¬ë¡¤ë§ ê²°ê³¼ ===")
    print(f"ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³  ìˆ˜: {len(jobs)}")
    
    for job in jobs[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
        print(f"\nì œëª©: {job['title']}")
        print(f"íšŒì‚¬: {job['company']}")
        print(f"ì§€ì—­: {job['location']}")
        print(f"ê¸‰ì—¬: {job['salary']}")
        print(f"ê³ ìš©í˜•íƒœ: {job['employment_type']}")
        print(f"URL: {job['url']}")


if __name__ == "__main__":
    main()