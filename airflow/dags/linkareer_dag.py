"""
Linkareer ìžê¸°ì†Œê°œì„œ í¬ë¡¤ë§ DAG
ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ìžê¸°ì†Œê°œì„œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
from datetime import datetime, timedelta
from airflow import DAG
try:
    from airflow.operators.python import PythonOperator
except ImportError:
    from airflow.operators.python_operator import PythonOperator

import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_path = '/home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform'
sys.path.append(project_path)
sys.path.append(os.path.join(project_path, 'crawling'))
sys.path.append(os.path.join(project_path, 'crawling', 'scrapers'))

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'mlops-platform',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 24),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
dag = DAG(
    'linkareer_cover_letter_crawl',
    default_args=default_args,
    description='Linkareer ìžê¸°ì†Œê°œì„œ ìˆ˜ì§‘ DAG',
    schedule_interval='0 3 * * *',  # ë§¤ì¼ ìƒˆë²½ 3ì‹œ
    catchup=False,
    tags=['crawling', 'cover-letter', 'linkareer'],
)


def run_linkareer_crawler(**context):
    """Linkareer í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    import sys
    import os
    
    try:
        # í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
        project_path = '/home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform'
        sys.path.insert(0, project_path)
        sys.path.insert(0, os.path.join(project_path, 'crawling'))
        sys.path.insert(0, os.path.join(project_path, 'crawling', 'scrapers'))
        
        from scrapers.linkareer_crawler import LinkareerCoverLetterCrawler
        
        print("=" * 80)
        print(f"Linkareer í¬ë¡¤ë§ ì‹œìž‘: {datetime.now()}")
        print("=" * 80)
        
        # í¬ë¡¤ëŸ¬ ì‹¤í–‰
        crawler = LinkareerCoverLetterCrawler()
        result = crawler.crawl(max_items=100)  # í•œ ë²ˆì— 100ê°œì”© ìˆ˜ì§‘
        
        print("\n" + "=" * 80)
        print(f"í¬ë¡¤ë§ ì™„ë£Œ: {datetime.now()}")
        print(f"ê²°ê³¼: {result}")
        print("=" * 80)
        
        # XComì— ê²°ê³¼ ì €ìž¥
        context['ti'].xcom_push(key='crawl_result', value=result)
        context['ti'].xcom_push(key='data_count', value=result.get('data_count', 0))
        
        return result
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise


def check_data_status(**context):
    """ìžê¸°ì†Œê°œì„œ ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ì ê²€"""
    from sqlalchemy import create_engine, text
    from dotenv import load_dotenv
    import os
    
    load_dotenv()
    DATABASE_URL = os.getenv("DATABASE_URL")
    
    if not DATABASE_URL:
        print("âŒ DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    engine = create_engine(DATABASE_URL)
    
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT COUNT(*) as total,
                   COUNT(DISTINCT company) as companies,
                   COUNT(CASE WHEN is_passed = true THEN 1 END) as passed
            FROM mlops.cover_letters
        """))
        
        row = result.fetchone()
        
        print("\n" + "=" * 80)
        print("ðŸ“Š ìžê¸°ì†Œê°œì„œ ìˆ˜ì§‘ í˜„í™©")
        print("=" * 80)
        print(f"  ì´ ìžê¸°ì†Œê°œì„œ: {row.total}ê±´")
        print(f"  ê³ ìœ  íšŒì‚¬: {row.companies}ê°œ")
        print(f"  í•©ê²© ìžì†Œì„œ: {row.passed}ê±´")
        print(f"  ëª©í‘œ ëŒ€ë¹„: {row.total / 1000 * 100:.1f}% (ëª©í‘œ 1,000ê±´)")
        print("=" * 80)
        
        # XComì— ìƒíƒœ ì €ìž¥
        ti = context['ti']
        ti.xcom_push(key='total_cover_letters', value=row.total)
        ti.xcom_push(key='total_companies', value=row.companies)
        ti.xcom_push(key='passed_count', value=row.passed)


def send_notification(**context):
    """í¬ë¡¤ë§ ê²°ê³¼ ì•Œë¦¼"""
    ti = context['ti']
    
    # ì´ì „ íƒœìŠ¤í¬ì˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    crawl_result = ti.xcom_pull(key='crawl_result', task_ids='run_linkareer_crawler')
    total_cover_letters = ti.xcom_pull(key='total_cover_letters', task_ids='check_status')
    
    print("\n" + "=" * 80)
    print("ðŸ“¢ Linkareer í¬ë¡¤ë§ ì™„ë£Œ ì•Œë¦¼")
    print("=" * 80)
    print(f"  í¬ë¡¤ë§ ê²°ê³¼: {crawl_result}")
    print(f"  ëˆ„ì  ë°ì´í„°: {total_cover_letters}ê±´")
    print(f"  ì§„í–‰ë¥ : {total_cover_letters / 1000 * 100:.1f}% (ëª©í‘œ 1,000ê±´)")
    print("=" * 80)


# íƒœìŠ¤í¬ ì •ì˜
task_run_crawler = PythonOperator(
    task_id='run_linkareer_crawler',
    python_callable=run_linkareer_crawler,
    provide_context=True,
    dag=dag,
)

task_check_status = PythonOperator(
    task_id='check_status',
    python_callable=check_data_status,
    provide_context=True,
    dag=dag,
)

task_send_notification = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    provide_context=True,
    dag=dag,
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
task_run_crawler >> task_check_status >> task_send_notification
