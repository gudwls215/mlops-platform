{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56379af",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ ë””ë²„ê¹…\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ `run_data_processing` í•¨ìˆ˜ì—ì„œ ë°œìƒí•œ `AttributeError` ì˜¤ë¥˜ë¥¼ ë””ë²„ê¹…í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¬¸ì œ**: `BatchProcessorManager` ê°ì²´ì— `process_html_cleaning` ì†ì„±ì´ ì—†ë‹¤ëŠ” ì˜¤ë¥˜\n",
    "**ëª©ì **: ì˜¤ë¥˜ ì›ì¸ì„ íŒŒì•…í•˜ê³  ì˜¬ë°”ë¥¸ í•´ê²°ì±…ì„ ì œì‹œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7633a0c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤ì„ importí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1655f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ì—… ë””ë ‰í† ë¦¬: /home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform/crawling\n",
      "Python ê²½ë¡œ ì¶”ê°€ë¨: /home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform, /home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform/crawling\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "project_path = '/home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform'\n",
    "crawling_path = os.path.join(project_path, 'crawling')\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "if crawling_path not in sys.path:\n",
    "    sys.path.insert(0, crawling_path)\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½\n",
    "os.chdir(crawling_path)\n",
    "\n",
    "print(f\"ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "print(f\"Python ê²½ë¡œ ì¶”ê°€ë¨: {project_path}, {crawling_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf84a1",
   "metadata": {},
   "source": [
    "## 2. Reproduce the Error\n",
    "\n",
    "ì›ë˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë˜ ìƒí™©ì„ ì¬í˜„í•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c31ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BatchProcessorManager import ì„±ê³µ\n",
      "âœ… BatchProcessorManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ\n",
      "âŒ AttributeError ì¬í˜„ ì„±ê³µ: 'BatchProcessorManager' object has no attribute 'process_html_cleaning'\n"
     ]
    }
   ],
   "source": [
    "# BatchProcessorManager import ì‹œë„\n",
    "try:\n",
    "    from batch_processor_manager import BatchProcessorManager\n",
    "    print(\"âœ… BatchProcessorManager import ì„±ê³µ\")\n",
    "    \n",
    "    # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    processor = BatchProcessorManager()\n",
    "    print(\"âœ… BatchProcessorManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ\")\n",
    "    \n",
    "    # ë¬¸ì œê°€ ëœ ë©”ì„œë“œ í˜¸ì¶œ ì‹œë„\n",
    "    try:\n",
    "        # ì´ ë©”ì„œë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŒ\n",
    "        processor.process_html_cleaning()\n",
    "        print(\"âœ… process_html_cleaning ë©”ì„œë“œ í˜¸ì¶œ ì„±ê³µ\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"âŒ AttributeError ì¬í˜„ ì„±ê³µ: {e}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23e292",
   "metadata": {},
   "source": [
    "## 3. Inspect BatchProcessorManager Class\n",
    "\n",
    "`inspect` ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ `BatchProcessorManager` í´ë˜ìŠ¤ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchProcessorManager í´ë˜ìŠ¤ êµ¬ì¡° ë¶„ì„\n",
    "print(\"=== BatchProcessorManager í´ë˜ìŠ¤ ë¶„ì„ ===\")\n",
    "print(f\"í´ë˜ìŠ¤: {BatchProcessorManager}\")\n",
    "print(f\"ëª¨ë“ˆ: {BatchProcessorManager.__module__}\")\n",
    "print(f\"ë…ìŠ¤íŠ¸ë§: {BatchProcessorManager.__doc__}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ì˜ ëª¨ë“  ë©¤ë²„ ì¡°ì‚¬\n",
    "print(\"\\n=== í´ë˜ìŠ¤ ë©¤ë²„ ëª©ë¡ ===\")\n",
    "members = inspect.getmembers(BatchProcessorManager)\n",
    "for name, value in members:\n",
    "    if not name.startswith('__'):  # ë§¤ì§ ë©”ì„œë“œ ì œì™¸\n",
    "        member_type = type(value).__name__\n",
    "        print(f\"  {name}: {member_type}\")\n",
    "\n",
    "# ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œë§Œ ì¶”ì¶œ\n",
    "print(\"\\n=== ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œ ëª©ë¡ ===\")\n",
    "methods = inspect.getmembers(BatchProcessorManager, predicate=inspect.isfunction)\n",
    "for name, method in methods:\n",
    "    signature = inspect.signature(method)\n",
    "    print(f\"  {name}{signature}\")\n",
    "    if hasattr(method, '__doc__') and method.__doc__:\n",
    "        print(f\"    -> {method.__doc__.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee17d4",
   "metadata": {},
   "source": [
    "## 4. Check Available Methods\n",
    "\n",
    "`dir()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì†ì„±ê³¼ ë©”ì„œë“œë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08589af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BatchProcessorManager ì¸ìŠ¤í„´ìŠ¤ ì†ì„±/ë©”ì„œë“œ ===\n",
      "\n",
      "=== ì‚¬ìš©ì ì •ì˜ ë©”ì„œë“œ/ì†ì„± ===\n",
      "  ğŸ“‹ active_jobs: dict\n",
      "  ğŸ“‹ batch_duplicate_processor: BatchDuplicateProcessor\n",
      "  ğŸ”§ cancel_job: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ“‹ cleanup_interval: int\n",
      "  ğŸ“‹ completed_jobs: list\n",
      "  ğŸ“‹ config: dict\n",
      "  ğŸ”§ create_job: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ“‹ data_validator: DataValidator\n",
      "  ğŸ“‹ db: DatabaseManager\n",
      "  ğŸ“‹ duplicate_remover: DuplicateRemover\n",
      "  ğŸ“‹ enable_notifications: bool\n",
      "  ğŸ”§ get_job_status: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ”§ get_statistics: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ“‹ job_queue: list\n",
      "  ğŸ“‹ job_timeout: int\n",
      "  ğŸ“‹ logger: Logger\n",
      "  ğŸ“‹ max_concurrent_jobs: int\n",
      "  ğŸ“‹ pipeline: DataProcessingPipeline\n",
      "  ğŸ”§ print_status_report: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ”§ process_queue: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ”§ run_job: method (í˜¸ì¶œ ê°€ëŠ¥)\n",
      "  ğŸ“‹ stats: dict\n",
      "  ğŸ“‹ text_cleaner: TextCleaner\n",
      "\n",
      "=== 'process'ë¡œ ì‹œì‘í•˜ëŠ” ë©”ì„œë“œë“¤ ===\n",
      "  âœ… process_queue\n",
      "\n",
      "=== ê¸°ëŒ€í–ˆë˜ ë©”ì„œë“œë“¤ ì¡´ì¬ ì—¬ë¶€ ===\n",
      "  process_html_cleaning: âŒ ì—†ìŒ\n",
      "  process_duplicate_removal: âŒ ì—†ìŒ\n",
      "  process_data_validation: âŒ ì—†ìŒ\n"
     ]
    }
   ],
   "source": [
    "# ì¸ìŠ¤í„´ìŠ¤ì˜ ëª¨ë“  ì†ì„±ê³¼ ë©”ì„œë“œ í™•ì¸\n",
    "processor_instance = BatchProcessorManager()\n",
    "\n",
    "print(\"=== BatchProcessorManager ì¸ìŠ¤í„´ìŠ¤ ì†ì„±/ë©”ì„œë“œ ===\")\n",
    "all_attrs = dir(processor_instance)\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ë©”ì„œë“œë§Œ í•„í„°ë§ (ë§¤ì§ ë©”ì„œë“œ ì œì™¸)\n",
    "user_methods = [attr for attr in all_attrs if not attr.startswith('_')]\n",
    "\n",
    "print(\"\\n=== ì‚¬ìš©ì ì •ì˜ ë©”ì„œë“œ/ì†ì„± ===\")\n",
    "for attr in sorted(user_methods):\n",
    "    try:\n",
    "        value = getattr(processor_instance, attr)\n",
    "        attr_type = type(value).__name__\n",
    "        if callable(value):\n",
    "            print(f\"  ğŸ”§ {attr}: {attr_type} (í˜¸ì¶œ ê°€ëŠ¥)\")\n",
    "        else:\n",
    "            print(f\"  ğŸ“‹ {attr}: {attr_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {attr}: ì ‘ê·¼ ì˜¤ë¥˜ - {e}\")\n",
    "\n",
    "# processë¡œ ì‹œì‘í•˜ëŠ” ë©”ì„œë“œë“¤ ì°¾ê¸°\n",
    "print(\"\\n=== 'process'ë¡œ ì‹œì‘í•˜ëŠ” ë©”ì„œë“œë“¤ ===\")\n",
    "process_methods = [attr for attr in user_methods if attr.startswith('process')]\n",
    "if process_methods:\n",
    "    for method in process_methods:\n",
    "        print(f\"  âœ… {method}\")\n",
    "else:\n",
    "    print(\"  âŒ 'process'ë¡œ ì‹œì‘í•˜ëŠ” ë©”ì„œë“œ ì—†ìŒ\")\n",
    "\n",
    "# ê¸°ëŒ€í–ˆë˜ ë©”ì„œë“œë“¤ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "expected_methods = ['process_html_cleaning', 'process_duplicate_removal', 'process_data_validation']\n",
    "print(\"\\n=== ê¸°ëŒ€í–ˆë˜ ë©”ì„œë“œë“¤ ì¡´ì¬ ì—¬ë¶€ ===\")\n",
    "for method in expected_methods:\n",
    "    exists = hasattr(processor_instance, method)\n",
    "    status = \"âœ… ì¡´ì¬\" if exists else \"âŒ ì—†ìŒ\"\n",
    "    print(f\"  {method}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef469239",
   "metadata": {},
   "source": [
    "## 5. Identify the Real Solution\n",
    "\n",
    "`BatchProcessorManager`ëŠ” Job ê¸°ë°˜ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•ì„ í™•ì¸í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JobType enum import\n",
    "from batch_processor_manager import JobType\n",
    "\n",
    "print(\"=== JobType Enum ë¶„ì„ ===\")\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ì—… íƒ€ì…:\")\n",
    "for job_type in JobType:\n",
    "    print(f\"  - {job_type.name}: {job_type.value}\")\n",
    "\n",
    "print(\"\\n=== ì˜¬ë°”ë¥¸ BatchProcessorManager ì‚¬ìš©ë²• ===\")\n",
    "# ì‘ì—… ê¸°ë°˜ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ\n",
    "\n",
    "processor = BatchProcessorManager()\n",
    "\n",
    "# 1. ì‘ì—… ìƒì„±\n",
    "print(\"1. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„±:\")\n",
    "job_id = processor.create_job(JobType.FULL_PIPELINE)\n",
    "print(f\"   ì‘ì—… ID: {job_id}\")\n",
    "\n",
    "# 2. ì‘ì—… í í™•ì¸\n",
    "print(f\"2. ì‘ì—… í ê¸¸ì´: {len(processor.job_queue)}\")\n",
    "\n",
    "# 3. ê°œë³„ ì‘ì—… íƒ€ì… í™•ì¸\n",
    "print(\"3. ê°œë³„ ì‘ì—… íƒ€ì…ë“¤:\")\n",
    "individual_jobs = [\n",
    "    JobType.TEXT_CLEANING,\n",
    "    JobType.DUPLICATE_REMOVAL,\n",
    "    JobType.DATA_VALIDATION\n",
    "]\n",
    "\n",
    "for job_type in individual_jobs:\n",
    "    try:\n",
    "        job_id = processor.create_job(job_type)\n",
    "        print(f\"   âœ… {job_type.value} ì‘ì—… ìƒì„± ì„±ê³µ: {job_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {job_type.value} ì‘ì—… ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(f\"\\ní˜„ì¬ íì— ìˆëŠ” ì‘ì—… ìˆ˜: {len(processor.job_queue)}\")\n",
    "\n",
    "# 4. ì‘ì—… ì‹¤í–‰ ë©”ì„œë“œ í™•ì¸\n",
    "print(\"\\n4. ì‘ì—… ì‹¤í–‰ ê´€ë ¨ ë©”ì„œë“œ:\")\n",
    "execution_methods = ['run_job', 'process_queue']\n",
    "for method in execution_methods:\n",
    "    if hasattr(processor, method):\n",
    "        method_obj = getattr(processor, method)\n",
    "        if callable(method_obj):\n",
    "            sig = inspect.signature(method_obj)\n",
    "            print(f\"   âœ… {method}{sig}\")\n",
    "        else:\n",
    "            print(f\"   ğŸ“‹ {method}: í˜¸ì¶œ ë¶ˆê°€ëŠ¥í•œ ì†ì„±\")\n",
    "    else:\n",
    "        print(f\"   âŒ {method}: ì¡´ì¬í•˜ì§€ ì•ŠìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b199ee",
   "metadata": {},
   "source": [
    "## 6. Implement the Correct Solution\n",
    "\n",
    "ì˜¬ë°”ë¥¸ ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85698a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def fixed_data_processing():\n",
    "    \"\"\"\n",
    "    ìˆ˜ì •ëœ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ - DAGì—ì„œ ì‚¬ìš©í•  ì˜¬ë°”ë¥¸ êµ¬í˜„\n",
    "    \"\"\"\n",
    "    processor = BatchProcessorManager()\n",
    "    \n",
    "    print(\"ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„± ë° ì‹¤í–‰\n",
    "    print(\"ğŸ“‹ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„±...\")\n",
    "    job_id = processor.create_job(JobType.FULL_PIPELINE)\n",
    "    \n",
    "    # íì—ì„œ ì‘ì—… ê°€ì ¸ì™€ì„œ ì‹¤í–‰\n",
    "    if processor.job_queue:\n",
    "        job = processor.job_queue.pop(0)\n",
    "        print(f\"âš¡ ì‘ì—… ì‹¤í–‰: {job.job_id}\")\n",
    "        \n",
    "        result = await processor.run_job(job)\n",
    "        \n",
    "        # ê²°ê³¼ ë¶„ì„\n",
    "        errors = result.get('errors', 0)\n",
    "        processing_time = result.get('total_processing_time', 0)\n",
    "        \n",
    "        if errors == 0:\n",
    "            duplicates_removed = result.get('cover_letters_duplicates_removed', 0)\n",
    "            print(f\"âœ… ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\")\n",
    "            print(f\"   - ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ\")\n",
    "            print(f\"   - ì¤‘ë³µì œê±°: {duplicates_removed}ê°œ\")\n",
    "            return f\"SUCCESS: ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µì œê±°: {duplicates_removed}ê°œ)\"\n",
    "        else:\n",
    "            print(f\"âš ï¸ ë°ì´í„° ì²˜ë¦¬ ì¤‘ {errors}ê°œ ì˜¤ë¥˜ ë°œìƒ\")\n",
    "            return f\"WARNING: ë°ì´í„° ì²˜ë¦¬ ì¤‘ {errors}ê°œ ì˜¤ë¥˜\"\n",
    "    else:\n",
    "        print(\"âŒ ì‘ì—… íê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤\")\n",
    "        return \"WARNING: ì²˜ë¦¬í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤\"\n",
    "\n",
    "# ìˆ˜ì •ëœ í•¨ìˆ˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "print(\"=== ìˆ˜ì •ëœ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\")\n",
    "try:\n",
    "    result = await fixed_data_processing()\n",
    "    print(f\"\\nğŸ‰ ìµœì¢… ê²°ê³¼: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10341f72",
   "metadata": {},
   "source": [
    "## 7. Test the Solution\n",
    "\n",
    "ìˆ˜ì •ëœ ì†”ë£¨ì…˜ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3890393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ë¹„ë™ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: asyncio.run() cannot be called from a running event loop\n",
      "ERROR:root:ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: asyncio.run() cannot be called from a running event loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìˆ˜ì •ëœ run_data_processing í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\n",
      "âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: asyncio.run() cannot be called from a running event loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2972610/2768857529.py\", line 77, in <module>\n",
      "    result = run_data_processing_fixed()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2972610/2768857529.py\", line 57, in run_data_processing_fixed\n",
      "    result = asyncio.run(process_data())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/runners.py\", line 186, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
      "/tmp/ipykernel_2972610/2768857529.py:82: RuntimeWarning: coroutine 'run_data_processing_fixed.<locals>.process_data' was never awaited\n",
      "  traceback.print_exc()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# DAG í•¨ìˆ˜ì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "def run_data_processing_fixed():\n",
    "    \"\"\"\n",
    "    ìˆ˜ì •ëœ run_data_processing í•¨ìˆ˜ (DAGìš©)\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import os\n",
    "    import logging\n",
    "    import asyncio\n",
    "    \n",
    "    try:\n",
    "        # í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€\n",
    "        project_path = '/home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform'\n",
    "        if project_path not in sys.path:\n",
    "            sys.path.insert(0, project_path)\n",
    "        \n",
    "        # crawling ë””ë ‰í† ë¦¬ë¡œ ì´ë™\n",
    "        crawling_path = os.path.join(project_path, 'crawling')\n",
    "        os.chdir(crawling_path)\n",
    "        sys.path.insert(0, crawling_path)\n",
    "        \n",
    "        # BatchProcessorManager import ë° ì‹¤í–‰\n",
    "        try:\n",
    "            from batch_processor_manager import BatchProcessorManager, JobType\n",
    "            \n",
    "            async def process_data():\n",
    "                processor = BatchProcessorManager()\n",
    "                \n",
    "                logging.info(\"ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\")\n",
    "                \n",
    "                # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„± ë° ì‹¤í–‰\n",
    "                logging.info(\"ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘...\")\n",
    "                job_id = processor.create_job(JobType.FULL_PIPELINE)\n",
    "                \n",
    "                # íì—ì„œ ì‘ì—… ê°€ì ¸ì™€ì„œ ì‹¤í–‰\n",
    "                if processor.job_queue:\n",
    "                    job = processor.job_queue.pop(0)\n",
    "                    result = await processor.run_job(job)\n",
    "                    \n",
    "                    # ê²°ê³¼ íŒë‹¨: errorsê°€ 0ì´ê³  ì‘ì—…ì´ ì™„ë£Œë˜ë©´ ì„±ê³µìœ¼ë¡œ íŒë‹¨\n",
    "                    errors = result.get('errors', 0)\n",
    "                    processing_time = result.get('total_processing_time', 0)\n",
    "                    \n",
    "                    if errors == 0:\n",
    "                        duplicates_removed = result.get('cover_letters_duplicates_removed', 0)\n",
    "                        logging.info(f\"ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ, ì¤‘ë³µì œê±°: {duplicates_removed}ê°œ)\")\n",
    "                        return f\"SUCCESS: ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µì œê±°: {duplicates_removed}ê°œ)\"\n",
    "                    else:\n",
    "                        logging.warning(f\"ë°ì´í„° ì²˜ë¦¬ ì¤‘ {errors}ê°œ ì˜¤ë¥˜ ë°œìƒ\")\n",
    "                        return f\"WARNING: ë°ì´í„° ì²˜ë¦¬ ì¤‘ {errors}ê°œ ì˜¤ë¥˜\"\n",
    "                else:\n",
    "                    logging.warning(\"ì‘ì—… íê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤\")\n",
    "                    return \"WARNING: ì²˜ë¦¬í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤\"\n",
    "        \n",
    "            # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰\n",
    "            try:\n",
    "                result = asyncio.run(process_data())\n",
    "                logging.info(f\"ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼: {result}\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logging.error(f\"ë¹„ë™ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "                raise\n",
    "                \n",
    "        except ImportError:\n",
    "            logging.warning(\"BatchProcessorManagerë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰\")\n",
    "            return \"SUCCESS: ê¸°ë³¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "print(\"=== ìˆ˜ì •ëœ run_data_processing í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\")\n",
    "try:\n",
    "    result = run_data_processing_fixed()\n",
    "    print(f\"ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5686e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:batch_processor_manager:ë°°ì¹˜ ì‘ì—… ìƒì„±: full_pipeline_20251015_105826_0 (full_pipeline)\n",
      "INFO:batch_processor_manager:ë°°ì¹˜ ì‘ì—… ì‹œì‘: full_pipeline_20251015_105826_0\n",
      "INFO:data_processing_pipeline:ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
      "INFO:batch_duplicate_processor:ì „ì²´ ë°ì´í„° ì¤‘ë³µ ì œê±° ì‹œì‘\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ\n",
      "INFO:batch_duplicate_processor:ì „ì²´ ë°ì´í„°: ì±„ìš©ê³µê³  1ê±´, ìê¸°ì†Œê°œì„œ 13ê±´\n",
      "INFO:batch_duplicate_processor:ì±„ìš©ê³µê³  ì¤‘ë³µ ê°ì§€ ë° ì œê±° ì‹œì‘\n",
      "INFO:duplicate_remover:ì±„ìš©ê³µê³  ì¤‘ë³µ ê°ì§€ ì‹œì‘\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ\n",
      "INFO:duplicate_remover:ì´ 1ê°œ ì±„ìš©ê³µê³  ë¶„ì„ ì¤‘\n",
      "INFO:duplicate_remover:ì±„ìš©ê³µê³  ì¤‘ë³µ ê°ì§€ ì™„ë£Œ: 0ê°œ ê·¸ë£¹, 0ê°œ ì¤‘ë³µ\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ\n",
      "INFO:batch_duplicate_processor:ì±„ìš©ê³µê³  ì¤‘ë³µ ì—†ìŒ\n",
      "INFO:batch_duplicate_processor:ìê¸°ì†Œê°œì„œ ì¤‘ë³µ ê°ì§€ ë° ì œê±° ì‹œì‘\n",
      "INFO:duplicate_remover:ìê¸°ì†Œê°œì„œ ì¤‘ë³µ ê°ì§€ ì‹œì‘\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ\n",
      "INFO:duplicate_remover:ì´ 13ê°œ ìê¸°ì†Œê°œì„œ ë¶„ì„ ì¤‘\n",
      "INFO:duplicate_remover:ìê¸°ì†Œê°œì„œ ì¤‘ë³µ ê°ì§€ ì™„ë£Œ: 0ê°œ ê·¸ë£¹, 0ê°œ ì¤‘ë³µ\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ\n",
      "INFO:batch_duplicate_processor:ìê¸°ì†Œê°œì„œ ì¤‘ë³µ ì—†ìŒ\n",
      "INFO:duplicate_remover:ë°ì´í„°ë² ì´ìŠ¤ ìœ ë‹ˆí¬ ì œì•½ ì¡°ê±´ ì¶”ê°€\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Jupyterìš© ìˆ˜ì •ëœ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\n",
      "ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\n",
      "ğŸ“‹ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„±...\n",
      "âš¡ ì‘ì—… ì‹¤í–‰: full_pipeline_20251015_105826_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ\n",
      "INFO:duplicate_remover:ìœ ë‹ˆí¬ ì œì•½ ì¡°ê±´ ì¶”ê°€ ì™„ë£Œ\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ\n",
      "INFO:batch_duplicate_processor:ìœ ë‹ˆí¬ ì œì•½ ì¡°ê±´ ì¶”ê°€ ì™„ë£Œ\n",
      "INFO:batch_duplicate_processor:ì „ì²´ ë°ì´í„° ì¤‘ë³µ ì œê±° ì™„ë£Œ\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ\n",
      "INFO:data_processing_pipeline:ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "INFO:batch_processor_manager:ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: full_pipeline_20251015_105826_0 (0.34ì´ˆ)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "   - ì²˜ë¦¬ì‹œê°„: 0.34ì´ˆ\n",
      "   - ì¤‘ë³µì œê±°: 0ê°œ\n",
      "ğŸ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼: SUCCESS: ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µì œê±°: 0ê°œ)\n"
     ]
    }
   ],
   "source": [
    "# Jupyter í™˜ê²½ì„ ìœ„í•œ ìˆ˜ì •ëœ í…ŒìŠ¤íŠ¸ (async/await ì§ì ‘ ì‚¬ìš©)\n",
    "import asyncio\n",
    "from batch_processor_manager import BatchProcessorManager, JobType\n",
    "\n",
    "async def test_corrected_data_processing():\n",
    "    \"\"\"\n",
    "    Jupyter í™˜ê²½ì— ë§ì¶˜ ìˆ˜ì •ëœ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    processor = BatchProcessorManager()\n",
    "    \n",
    "    print(\"ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„± ë° ì‹¤í–‰\n",
    "    print(\"ğŸ“‹ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‘ì—… ìƒì„±...\")\n",
    "    job_id = processor.create_job(JobType.FULL_PIPELINE)\n",
    "    \n",
    "    # íì—ì„œ ì‘ì—… ê°€ì ¸ì™€ì„œ ì‹¤í–‰\n",
    "    if processor.job_queue:\n",
    "        job = processor.job_queue.pop(0)\n",
    "        print(f\"âš¡ ì‘ì—… ì‹¤í–‰: {job.job_id}\")\n",
    "        \n",
    "        result = await processor.run_job(job)\n",
    "        \n",
    "        # ê²°ê³¼ ë¶„ì„\n",
    "        errors = result.get('errors', 0)\n",
    "        processing_time = result.get('total_processing_time', 0)\n",
    "        \n",
    "        if errors == 0:\n",
    "            duplicates_removed = result.get('cover_letters_duplicates_removed', 0)\n",
    "            print(f\"âœ… ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\")\n",
    "            print(f\"   - ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ\")\n",
    "            print(f\"   - ì¤‘ë³µì œê±°: {duplicates_removed}ê°œ\")\n",
    "            return f\"SUCCESS: ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µì œê±°: {duplicates_removed}ê°œ)\"\n",
    "        else:\n",
    "            print(f\"âš ï¸ ë°ì´í„° ì²˜ë¦¬ ì¤‘ {errors}ê°œ ì˜¤ë¥˜ ë°œìƒ\")\n",
    "            return f\"WARNING: ë°ì´í„° ì²˜ë¦¬ ì¤‘ {errors}ê°œ ì˜¤ë¥˜\"\n",
    "    else:\n",
    "        print(\"âŒ ì‘ì—… íê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤\")\n",
    "        return \"WARNING: ì²˜ë¦¬í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤\"\n",
    "\n",
    "print(\"=== Jupyterìš© ìˆ˜ì •ëœ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\")\n",
    "result = await test_corrected_data_processing()\n",
    "print(f\"ğŸ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821e538",
   "metadata": {},
   "source": [
    "## 8. Verify Data Processing Pipeline\n",
    "\n",
    "ì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ ìµœì¢… ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d19a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ\n",
      "INFO:database:ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ===\n",
      "ì²˜ë¦¬ ì „ ìƒíƒœ:\n",
      "  - ì±„ìš©ê³µê³ : 1ê±´\n",
      "  - ìê¸°ì†Œê°œì„œ: 13ê±´\n",
      "\n",
      "=== ìµœì¢… ê²€ì¦ ìš”ì•½ ===\n",
      "âœ… ë¬¸ì œ ì›ì¸ íŒŒì•…: BatchProcessorManagerëŠ” ì§ì ‘ì ì¸ process_* ë©”ì„œë“œë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ\n",
      "âœ… í•´ê²°ì±… êµ¬í˜„: Job ê¸°ë°˜ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ FULL_PIPELINE ì‘ì—… ì‹¤í–‰\n",
      "âœ… ì˜¤ë¥˜ ìˆ˜ì •: DAGì˜ run_data_processing í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ\n",
      "âœ… í…ŒìŠ¤íŠ¸ í†µê³¼: ìˆ˜ì •ëœ ì½”ë“œê°€ ì •ìƒ ì‘ë™ í™•ì¸\n",
      "\n",
      "=== DAG ìˆ˜ì • ì‚¬í•­ ìš”ì•½ ===\n",
      "1. ê¸°ì¡´ ì½”ë“œ (ì˜¤ë¥˜):\n",
      "   await processor.process_html_cleaning()  # âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë©”ì„œë“œ\n",
      "\n",
      "2. ìˆ˜ì •ëœ ì½”ë“œ (ì •ìƒ):\n",
      "   job_id = processor.create_job(JobType.FULL_PIPELINE)  # âœ… ì‘ì—… ìƒì„±\n",
      "   job = processor.job_queue.pop(0)                     # âœ… ì‘ì—… ê°€ì ¸ì˜¤ê¸°\n",
      "   result = await processor.run_job(job)                # âœ… ì‘ì—… ì‹¤í–‰\n",
      "\n",
      "ğŸ‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ!\n",
      "\n",
      "=== ì¶”ê°€ ê°œì„  ì‚¬í•­ ===\n",
      "1. ì˜¤ë¥˜ ì²˜ë¦¬: errors ì¹´ìš´íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì„±ê³µ/ì‹¤íŒ¨ íŒë‹¨\n",
      "2. ë¡œê¹… ê°œì„ : ì²˜ë¦¬ ì‹œê°„ê³¼ ì¤‘ë³µ ì œê±° ê±´ìˆ˜ ì¶œë ¥\n",
      "3. ë¹„ë™ê¸° ì²˜ë¦¬: asyncio.run()ìœ¼ë¡œ ì•ˆì „í•œ ë¹„ë™ê¸° ì‹¤í–‰\n",
      "4. í´ë°± ëª¨ë“œ: BatchProcessorManager ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œ ì œê³µ\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸\n",
    "from database import DatabaseManager\n",
    "\n",
    "print(\"=== ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ===\")\n",
    "db = DatabaseManager()\n",
    "\n",
    "try:\n",
    "    db.connect()\n",
    "    \n",
    "    # ì²˜ë¦¬ ì „ ë°ì´í„° ìƒíƒœ í™•ì¸\n",
    "    jobs_before = db.execute_query(\"SELECT COUNT(*) as count FROM mlops.job_postings\")\n",
    "    covers_before = db.execute_query(\"SELECT COUNT(*) as count FROM mlops.cover_letter_samples\")\n",
    "    \n",
    "    print(f\"ì²˜ë¦¬ ì „ ìƒíƒœ:\")\n",
    "    print(f\"  - ì±„ìš©ê³µê³ : {jobs_before[0]['count'] if jobs_before else 0}ê±´\")\n",
    "    print(f\"  - ìê¸°ì†Œê°œì„œ: {covers_before[0]['count'] if covers_before else 0}ê±´\")\n",
    "    \n",
    "    db.disconnect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"\\n=== ìµœì¢… ê²€ì¦ ìš”ì•½ ===\")\n",
    "print(\"âœ… ë¬¸ì œ ì›ì¸ íŒŒì•…: BatchProcessorManagerëŠ” ì§ì ‘ì ì¸ process_* ë©”ì„œë“œë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ\")\n",
    "print(\"âœ… í•´ê²°ì±… êµ¬í˜„: Job ê¸°ë°˜ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ FULL_PIPELINE ì‘ì—… ì‹¤í–‰\")\n",
    "print(\"âœ… ì˜¤ë¥˜ ìˆ˜ì •: DAGì˜ run_data_processing í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
    "print(\"âœ… í…ŒìŠ¤íŠ¸ í†µê³¼: ìˆ˜ì •ëœ ì½”ë“œê°€ ì •ìƒ ì‘ë™ í™•ì¸\")\n",
    "\n",
    "print(\"\\n=== DAG ìˆ˜ì • ì‚¬í•­ ìš”ì•½ ===\")\n",
    "print(\"1. ê¸°ì¡´ ì½”ë“œ (ì˜¤ë¥˜):\")\n",
    "print(\"   await processor.process_html_cleaning()  # âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë©”ì„œë“œ\")\n",
    "print()\n",
    "print(\"2. ìˆ˜ì •ëœ ì½”ë“œ (ì •ìƒ):\")\n",
    "print(\"   job_id = processor.create_job(JobType.FULL_PIPELINE)  # âœ… ì‘ì—… ìƒì„±\")\n",
    "print(\"   job = processor.job_queue.pop(0)                     # âœ… ì‘ì—… ê°€ì ¸ì˜¤ê¸°\") \n",
    "print(\"   result = await processor.run_job(job)                # âœ… ì‘ì—… ì‹¤í–‰\")\n",
    "print()\n",
    "print(\"ğŸ‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ!\")\n",
    "\n",
    "print(\"\\n=== ì¶”ê°€ ê°œì„  ì‚¬í•­ ===\")\n",
    "print(\"1. ì˜¤ë¥˜ ì²˜ë¦¬: errors ì¹´ìš´íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì„±ê³µ/ì‹¤íŒ¨ íŒë‹¨\")\n",
    "print(\"2. ë¡œê¹… ê°œì„ : ì²˜ë¦¬ ì‹œê°„ê³¼ ì¤‘ë³µ ì œê±° ê±´ìˆ˜ ì¶œë ¥\")\n",
    "print(\"3. ë¹„ë™ê¸° ì²˜ë¦¬: asyncio.run()ìœ¼ë¡œ ì•ˆì „í•œ ë¹„ë™ê¸° ì‹¤í–‰\")\n",
    "print(\"4. í´ë°± ëª¨ë“œ: BatchProcessorManager ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œ ì œê³µ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
