#!/usr/bin/env python3
"""
Saramin í¬ë¡¤ëŸ¬ ë‹¨ë… í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
íŠ¸ëœì­ì…˜ ì¶©ëŒ ì—†ì´ Saraminë§Œ ì‹¤í–‰
"""

import sys
import os
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_path = '/home/ttm/tensorflow-jupyter/jupyterNotebook/khj/mlops-platform'
sys.path.insert(0, project_path)
sys.path.insert(0, os.path.join(project_path, 'crawling'))
sys.path.insert(0, os.path.join(project_path, 'crawling', 'scrapers'))

def test_saramin_only():
    """Saramin í¬ë¡¤ëŸ¬ ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ Saramin í¬ë¡¤ëŸ¬ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    
    try:
        # ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
        os.chdir(os.path.join(project_path, 'crawling', 'scrapers'))
        
        # Saramin í¬ë¡¤ëŸ¬ import
        from saramin_crawler import SaraminCrawler
        
        print("ğŸ“Š Saramin í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
        crawler = SaraminCrawler()
        
        print("ğŸš€ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ 15ê°œ, ì‹¤ì‹œê°„ DB ì €ì¥)")
        print("-"*40)
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        result = crawler.crawl_jobs(max_jobs=15, save_to_db=True)
        
        print("-"*40)
        print("ğŸ“‹ ê²°ê³¼ ìš”ì•½:")
        print(f"   ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³ : {len(result)}ê°œ")
        
        if result:
            print("âœ… í¬ë¡¤ë§ ë° DB ì €ì¥ ì™„ë£Œ!")
            
            # ê°„ë‹¨í•œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            print("\nğŸ“ ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
            for i, job in enumerate(result[:3], 1):
                print(f"   {i}. {job['title'][:50]}... - {job['company']}")
        else:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        return len(result)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 0

def check_db_after_test():
    """í…ŒìŠ¤íŠ¸ í›„ DB í™•ì¸"""
    print("\nğŸ” DB ì €ì¥ ê²°ê³¼ í™•ì¸")
    print("-"*30)
    
    try:
        from database import DatabaseManager
        
        db = DatabaseManager()
        db.connect()
        
        # ìµœê·¼ 1ì‹œê°„ ë°ì´í„° ì¡°íšŒ
        recent_data = db.execute_query("""
            SELECT id, title, company, source, created_at 
            FROM mlops.job_postings 
            WHERE created_at >= NOW() - INTERVAL '1 hour'
            AND source = 'saramin'
            ORDER BY id DESC
            LIMIT 10
        """)
        
        print(f"ğŸ“Š ìµœê·¼ 1ì‹œê°„ê°„ Saramin ë°ì´í„°: {len(recent_data)}ê°œ")
        
        for job in recent_data[:5]:
            print(f"   ID: {job['id']}, ì œëª©: {job['title'][:40]}..., íšŒì‚¬: {job['company']}")
        
        # ì „ì²´ í†µê³„
        total_stats = db.execute_query("""
            SELECT source, COUNT(*) as count 
            FROM mlops.job_postings 
            GROUP BY source
        """)
        
        print("\nğŸ“ˆ ì „ì²´ DB í†µê³„:")
        for stat in total_stats:
            print(f"   {stat['source']}: {stat['count']}ê°œ")
            
        return len(recent_data)
        
    except Exception as e:
        print(f"âŒ DB í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

if __name__ == "__main__":
    start_time = datetime.now()
    
    # 1. Saramin í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    crawled_count = test_saramin_only()
    
    # 2. DB í™•ì¸
    saved_count = check_db_after_test()
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    print(f"   ì‹¤í–‰ ì‹œê°„: {duration:.1f}ì´ˆ")
    print(f"   í¬ë¡¤ë§: {crawled_count}ê°œ")
    print(f"   DB ì €ì¥: {saved_count}ê°œ")
    
    if crawled_count > 0 and saved_count > 0:
        print("âœ… Saramin í¬ë¡¤ëŸ¬ ë‹¨ë… ì‹¤í–‰ ì„±ê³µ!")
    else:
        print("âŒ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")