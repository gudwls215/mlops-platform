"""
ì¶”ì²œ ë‹¤ì–‘ì„± ë° ì°¸ì‹ ì„± ì•Œê³ ë¦¬ì¦˜
MMR (Maximal Marginal Relevance) ë° Novelty Score ê¸°ë°˜
"""

import numpy as np
from typing import List, Dict, Tuple
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import os
from dotenv import load_dotenv

load_dotenv()

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
DB_HOST = os.getenv('DATABASE_HOST', '114.202.2.226')
DB_PORT = os.getenv('DATABASE_PORT', '5433')
DB_NAME = os.getenv('DATABASE_NAME', 'postgres')
DB_USER = os.getenv('DATABASE_USER', 'postgres')
DB_PASSWORD = os.getenv('DATABASE_PASSWORD', '')
DB_SCHEMA = 'mlops'

import urllib.parse
encoded_password = urllib.parse.quote_plus(DB_PASSWORD)
DATABASE_URL = f"postgresql://{DB_USER}:{encoded_password}@{DB_HOST}:{DB_PORT}/{DB_NAME}"


class DiversityNoveltyReranker:
    """ì¶”ì²œ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ê³¼ ì°¸ì‹ ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¬ì •ë ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.engine = create_engine(DATABASE_URL)
    
    def mmr_rerank(
        self,
        recommendations: List[Dict],
        resume_embedding: np.ndarray,
        job_embeddings: Dict[int, np.ndarray],
        lambda_param: float = 0.5,
        top_n: int = 10
    ) -> List[Dict]:
        """
        MMR (Maximal Marginal Relevance) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¬ì •ë ¬
        
        Args:
            recommendations: ì¶”ì²œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{"job_id": int, "similarity": float, ...}, ...]
            resume_embedding: ì´ë ¥ì„œ ì„ë² ë”© ë²¡í„°
            job_embeddings: {job_id: embedding} ë”•ì…”ë„ˆë¦¬
            lambda_param: ìœ ì‚¬ë„ì™€ ë‹¤ì–‘ì„± ê°„ì˜ ê· í˜• íŒŒë¼ë¯¸í„° (0~1)
                         1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ë„ ì¤‘ì‹œ, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¤‘ì‹œ
            top_n: ìµœì¢… ì„ íƒí•  ì¶”ì²œ ê°œìˆ˜
        
        Returns:
            ì¬ì •ë ¬ëœ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
        """
        if not recommendations or len(recommendations) == 0:
            return []
        
        # ì„ë² ë”©ì´ ì—†ëŠ” í•­ëª© ì œê±°
        valid_recommendations = [
            rec for rec in recommendations
            if rec['job_id'] in job_embeddings
        ]
        
        if not valid_recommendations:
            return recommendations[:top_n]
        
        # ì´ˆê¸° ìœ ì‚¬ë„ ì ìˆ˜ ì •ê·œí™”
        scores = np.array([rec.get('similarity', rec.get('cf_score', 0)) 
                          for rec in valid_recommendations])
        if scores.max() > 0:
            normalized_scores = scores / scores.max()
        else:
            normalized_scores = scores
        
        # ì„ íƒëœ í•­ëª©ê³¼ í›„ë³´ í•­ëª©
        selected = []
        candidates = list(range(len(valid_recommendations)))
        selected_embeddings = []
        
        resume_emb_2d = resume_embedding.reshape(1, -1)
        
        # ì²« ë²ˆì§¸ í•­ëª©ì€ ìµœê³  ì ìˆ˜ë¡œ ì„ íƒ
        first_idx = candidates[np.argmax(normalized_scores)]
        selected.append(first_idx)
        candidates.remove(first_idx)
        selected_embeddings.append(job_embeddings[valid_recommendations[first_idx]['job_id']])
        
        # ë‚˜ë¨¸ì§€ í•­ëª© ì„ íƒ (MMR)
        while len(selected) < top_n and candidates:
            mmr_scores = []
            
            for idx in candidates:
                job_id = valid_recommendations[idx]['job_id']
                job_emb = job_embeddings[job_id].reshape(1, -1)
                
                # ìœ ì‚¬ë„ (ì´ë ¥ì„œì™€ì˜ ìœ ì‚¬ë„)
                relevance = cosine_similarity(resume_emb_2d, job_emb)[0][0]
                
                # ë‹¤ì–‘ì„± (ì´ë¯¸ ì„ íƒëœ í•­ëª©ë“¤ê³¼ì˜ ìµœëŒ€ ìœ ì‚¬ë„)
                if selected_embeddings:
                    selected_emb_matrix = np.vstack(selected_embeddings)
                    max_similarity = cosine_similarity(job_emb, selected_emb_matrix)[0].max()
                else:
                    max_similarity = 0
                
                # MMR ì ìˆ˜ ê³„ì‚°
                mmr = lambda_param * relevance - (1 - lambda_param) * max_similarity
                mmr_scores.append(mmr)
            
            # ìµœê³  MMR ì ìˆ˜ ì„ íƒ
            best_idx = candidates[np.argmax(mmr_scores)]
            selected.append(best_idx)
            candidates.remove(best_idx)
            selected_embeddings.append(job_embeddings[valid_recommendations[best_idx]['job_id']])
        
        # ì¬ì •ë ¬ëœ ê²°ê³¼ ë°˜í™˜
        reranked = [valid_recommendations[idx] for idx in selected]
        
        # MMR ìˆœìœ„ ì¶”ê°€
        for rank, rec in enumerate(reranked, 1):
            rec['mmr_rank'] = rank
        
        return reranked
    
    def calculate_novelty_scores(
        self,
        recommendations: List[Dict],
        user_id: int,
        time_decay_days: int = 30
    ) -> List[Dict]:
        """
        ì¶”ì²œ í•­ëª©ì˜ ì°¸ì‹ ì„±(Novelty) ì ìˆ˜ ê³„ì‚°
        
        Args:
            recommendations: ì¶”ì²œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            user_id: ì‚¬ìš©ì ID
            time_decay_days: ì°¸ì‹ ì„± ê°ì‡  ê¸°ê°„ (ì¼)
        
        Returns:
            novelty_scoreê°€ ì¶”ê°€ëœ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” calculate_novelty_scores í˜¸ì¶œ: user_id={user_id}, recommendations={len(recommendations)}ê°œ")
        
        if not recommendations:
            return recommendations
        
        job_ids = [rec['job_id'] for rec in recommendations]
        
        # ì‚¬ìš©ìê°€ ë³¸ ì±„ìš©ê³µê³  ì¡°íšŒ
        viewed_query = f"""
        SELECT job_id, MAX(interaction_time) as last_viewed
        FROM {DB_SCHEMA}.user_interactions
        WHERE user_id = :user_id 
          AND job_id = ANY(:job_ids)
          AND interaction_type IN ('view', 'click')
        GROUP BY job_id
        """
        
        try:
            with self.engine.connect() as conn:
                viewed_df = pd.read_sql(
                    text(viewed_query),
                    conn,
                    params={"user_id": user_id, "job_ids": job_ids}
                )
        except Exception as e:
            # user_interactions í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ
            print(f"âš ï¸ user_interactions ì¡°íšŒ ì‹¤íŒ¨: {e}")
            viewed_df = pd.DataFrame(columns=['job_id', 'last_viewed'])
        
        viewed_dict = dict(zip(viewed_df['job_id'], viewed_df['last_viewed']))
        
        # ì±„ìš©ê³µê³  ë“±ë¡ì¼ ì¡°íšŒ (created_at ì‚¬ìš©)
        posted_query = f"""
        SELECT id, created_at
        FROM {DB_SCHEMA}.job_postings
        WHERE id = ANY(:job_ids)
        """
        
        try:
            with self.engine.connect() as conn:
                posted_df = pd.read_sql(
                    text(posted_query),
                    conn,
                    params={"job_ids": job_ids}
                )
        except Exception as e:
            print(f"âš ï¸ job_postings created_at ì¡°íšŒ ì‹¤íŒ¨: {e}")
            posted_df = pd.DataFrame(columns=['id', 'created_at'])
        
        posted_dict = dict(zip(posted_df['id'], posted_df['created_at']))
        
        current_time = datetime.now()
        
        # ê° ì¶”ì²œ í•­ëª©ì˜ ì°¸ì‹ ì„± ì ìˆ˜ ê³„ì‚°
        for rec in recommendations:
            job_id = rec['job_id']
            
            # 1. ì‚¬ìš©ìê°€ ë³¸ ì ì´ ìˆëŠ”ì§€ í™•ì¸
            if job_id in viewed_dict:
                # ë³¸ ì ì´ ìˆìœ¼ë©´ ì‹œê°„ ê²½ê³¼ì— ë”°ë¼ ì ìˆ˜ ë¶€ì—¬
                last_viewed = viewed_dict[job_id]
                if isinstance(last_viewed, str):
                    last_viewed = datetime.fromisoformat(last_viewed)
                # Timezone awareì¸ ê²½ìš° naiveë¡œ ë³€í™˜
                if hasattr(last_viewed, 'tzinfo') and last_viewed.tzinfo is not None:
                    last_viewed = last_viewed.replace(tzinfo=None)
                
                days_since_view = (current_time - last_viewed).days
                
                # ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ê°ì‡  (0~1)
                time_factor = min(days_since_view / time_decay_days, 1.0)
                user_novelty = time_factor
            else:
                # ë³¸ ì ì´ ì—†ìœ¼ë©´ ì™„ì „íˆ ìƒˆë¡œìš´ í•­ëª©
                user_novelty = 1.0
            
            # 2. ì±„ìš©ê³µê³ ì˜ ë“±ë¡ì¼ í™•ì¸ (ìµœê·¼ ê³µê³ ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            if job_id in posted_dict:
                posted_date = posted_dict[job_id]
                if isinstance(posted_date, str):
                    posted_date = datetime.fromisoformat(posted_date)
                # Timezone awareì¸ ê²½ìš° naiveë¡œ ë³€í™˜
                if hasattr(posted_date, 'tzinfo') and posted_date.tzinfo is not None:
                    posted_date = posted_date.replace(tzinfo=None)
                
                days_since_posted = (current_time - posted_date).days
                
                # ìµœê·¼ 30ì¼ ì´ë‚´ëŠ” 1.0, ê·¸ ì´í›„ëŠ” ì„ í˜• ê°ì†Œ (ìµœì†Œ 0.5)
                if days_since_posted <= 30:
                    recency_factor = 1.0
                else:
                    recency_factor = max(0.5, 1.0 - (days_since_posted - 30) / 180)
            else:
                recency_factor = 0.7  # ê¸°ë³¸ê°’
            
            # 3. íšŒì‚¬/ì§ë¬´ ë‹¤ì–‘ì„± (ê°„ë‹¨í•œ ë²„ì „: ì¶”í›„ í™•ì¥ ê°€ëŠ¥)
            # í˜„ì¬ëŠ” ì‚¬ìš©ì noveltyì™€ recencyë¥¼ ê²°í•©
            novelty_score = (user_novelty * 0.6 + recency_factor * 0.4)
            
            rec['novelty_score'] = novelty_score
            rec['user_novelty'] = user_novelty
            rec['recency_factor'] = recency_factor
            
            print(f"  Job {job_id}: novelty={novelty_score:.3f}, user_novelty={user_novelty:.3f}, recency={recency_factor:.3f}")
        
        print(f"âœ… calculate_novelty_scores ì™„ë£Œ: {len(recommendations)}ê°œ ì²˜ë¦¬")
        return recommendations
    
    def hybrid_rerank(
        self,
        recommendations: List[Dict],
        resume_embedding: np.ndarray,
        job_embeddings: Dict[int, np.ndarray],
        user_id: int,
        diversity_weight: float = 0.3,
        novelty_weight: float = 0.2,
        relevance_weight: float = 0.5,
        mmr_lambda: float = 0.7,
        top_n: int = 10
    ) -> List[Dict]:
        """
        ë‹¤ì–‘ì„±, ì°¸ì‹ ì„±, ì—°ê´€ì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ í•˜ì´ë¸Œë¦¬ë“œ ì¬ì •ë ¬
        
        Args:
            recommendations: ì¶”ì²œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            resume_embedding: ì´ë ¥ì„œ ì„ë² ë”©
            job_embeddings: ì±„ìš©ê³µê³  ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
            user_id: ì‚¬ìš©ì ID
            diversity_weight: ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜
            novelty_weight: ì°¸ì‹ ì„± ê°€ì¤‘ì¹˜
            relevance_weight: ì—°ê´€ì„± ê°€ì¤‘ì¹˜
            mmr_lambda: MMR ì•Œê³ ë¦¬ì¦˜ì˜ lambda íŒŒë¼ë¯¸í„°
            top_n: ìµœì¢… ê²°ê³¼ ê°œìˆ˜
        
        Returns:
            ìµœì¢… ì¬ì •ë ¬ëœ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸
        """
        if not recommendations:
            return []
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = diversity_weight + novelty_weight + relevance_weight
        diversity_weight /= total_weight
        novelty_weight /= total_weight
        relevance_weight /= total_weight
        
        # 1. MMRë¡œ ë‹¤ì–‘ì„± ìˆëŠ” top 2N ì„ íƒ
        mmr_candidates = self.mmr_rerank(
            recommendations=recommendations,
            resume_embedding=resume_embedding,
            job_embeddings=job_embeddings,
            lambda_param=mmr_lambda,
            top_n=min(top_n * 2, len(recommendations))
        )
        
        # 2. ì°¸ì‹ ì„± ì ìˆ˜ ê³„ì‚°
        mmr_candidates = self.calculate_novelty_scores(
            recommendations=mmr_candidates,
            user_id=user_id
        )
        
        # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        for rec in mmr_candidates:
            # ì—°ê´€ì„± ì ìˆ˜ (ì›ë˜ ì ìˆ˜)
            relevance = rec.get('similarity', rec.get('cf_score', 0))
            
            # ë‹¤ì–‘ì„± ì ìˆ˜ (MMR ìˆœìœ„ ê¸°ë°˜: ìƒìœ„ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            mmr_rank = rec.get('mmr_rank', len(mmr_candidates))
            diversity_score = 1.0 - (mmr_rank - 1) / len(mmr_candidates)
            
            # ì°¸ì‹ ì„± ì ìˆ˜
            novelty = rec.get('novelty_score', 0.5)
            
            # ìµœì¢… ì ìˆ˜
            final_score = (
                relevance_weight * relevance +
                diversity_weight * diversity_score +
                novelty_weight * novelty
            )
            
            rec['final_score'] = final_score
            rec['diversity_score'] = diversity_score
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        mmr_candidates.sort(key=lambda x: x['final_score'], reverse=True)
        
        return mmr_candidates[:top_n]
    
    def analyze_diversity(
        self,
        recommendations: List[Dict],
        job_embeddings: Dict[int, np.ndarray]
    ) -> Dict:
        """
        ì¶”ì²œ ê²°ê³¼ì˜ ë‹¤ì–‘ì„± ë¶„ì„
        
        Returns:
            {
                "avg_similarity": float,  # ì¶”ì²œ í•­ëª© ê°„ í‰ê·  ìœ ì‚¬ë„
                "min_similarity": float,  # ìµœì†Œ ìœ ì‚¬ë„
                "max_similarity": float,  # ìµœëŒ€ ìœ ì‚¬ë„
                "diversity_score": float  # ë‹¤ì–‘ì„± ì ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
            }
        """
        if len(recommendations) < 2:
            return {
                "avg_similarity": 0.0,
                "min_similarity": 0.0,
                "max_similarity": 0.0,
                "diversity_score": 1.0
            }
        
        # ì„ë² ë”© ì¶”ì¶œ
        embeddings = []
        for rec in recommendations:
            if rec['job_id'] in job_embeddings:
                embeddings.append(job_embeddings[rec['job_id']])
        
        if len(embeddings) < 2:
            return {
                "avg_similarity": 0.0,
                "min_similarity": 0.0,
                "max_similarity": 0.0,
                "diversity_score": 1.0
            }
        
        # ì¶”ì²œ í•­ëª© ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        embeddings_matrix = np.vstack(embeddings)
        similarity_matrix = cosine_similarity(embeddings_matrix)
        
        # ëŒ€ê°ì„  ì œì™¸ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„)
        mask = np.ones(similarity_matrix.shape, dtype=bool)
        np.fill_diagonal(mask, False)
        similarities = similarity_matrix[mask]
        
        avg_sim = similarities.mean()
        min_sim = similarities.min()
        max_sim = similarities.max()
        
        # ë‹¤ì–‘ì„± ì ìˆ˜ (1 - í‰ê·  ìœ ì‚¬ë„)
        diversity_score = 1.0 - avg_sim
        
        return {
            "avg_similarity": float(avg_sim),
            "min_similarity": float(min_sim),
            "max_similarity": float(max_sim),
            "diversity_score": float(diversity_score)
        }


def parse_embedding(embedding_str: str) -> np.ndarray:
    """ì„ë² ë”© ë¬¸ìì—´ íŒŒì‹±"""
    if embedding_str is None:
        return None
    embedding_str = embedding_str.strip('[]')
    values = [float(x.strip()) for x in embedding_str.split(',')]
    return np.array(values)


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì½”ë“œ"""
    from sqlalchemy import create_engine, text
    
    print("=" * 60)
    print("ì¶”ì²œ ë‹¤ì–‘ì„± ë° ì°¸ì‹ ì„± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    reranker = DiversityNoveltyReranker()
    engine = create_engine(DATABASE_URL)
    
    # í…ŒìŠ¤íŠ¸ìš© ì´ë ¥ì„œ ID
    test_resume_id = 1
    
    # ì´ë ¥ì„œ ì„ë² ë”© ì¡°íšŒ
    resume_query = f"""
    SELECT id, embedding_array
    FROM {DB_SCHEMA}.cover_letter_samples
    WHERE id = :resume_id
    """
    
    with engine.connect() as conn:
        resume_result = conn.execute(text(resume_query), {"resume_id": test_resume_id}).fetchone()
    
    if not resume_result:
        print(f"âŒ ì´ë ¥ì„œ ID {test_resume_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)
    
    resume_embedding = parse_embedding(resume_result[1])
    
    # ìƒìœ„ 50ê°œ ì±„ìš©ê³µê³  ì¡°íšŒ
    jobs_query = f"""
    SELECT id, title, company, embedding_array
    FROM {DB_SCHEMA}.job_postings
    WHERE embedding_array IS NOT NULL
    LIMIT 50
    """
    
    with engine.connect() as conn:
        jobs_df = pd.read_sql(text(jobs_query), conn)
    
    # ì„ë² ë”© íŒŒì‹± ë° ìœ ì‚¬ë„ ê³„ì‚°
    job_embeddings = {}
    recommendations = []
    
    resume_emb_2d = resume_embedding.reshape(1, -1)
    
    for _, row in jobs_df.iterrows():
        emb = parse_embedding(row['embedding_array'])
        if emb is not None:
            job_embeddings[row['id']] = emb
            similarity = cosine_similarity(resume_emb_2d, emb.reshape(1, -1))[0][0]
            recommendations.append({
                "job_id": row['id'],
                "title": row['title'],
                "company": row['company'],
                "similarity": similarity
            })
    
    # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
    recommendations.sort(key=lambda x: x['similarity'], reverse=True)
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: ì´ë ¥ì„œ ID {test_resume_id}, ì±„ìš©ê³µê³  {len(recommendations)}ê°œ\n")
    
    # 1. ê¸°ë³¸ ì¶”ì²œ (Top 10)
    print("=" * 60)
    print("1. ê¸°ë³¸ ì¶”ì²œ (ìœ ì‚¬ë„ ê¸°ì¤€ Top 10)")
    print("=" * 60)
    baseline = recommendations[:10]
    for i, rec in enumerate(baseline, 1):
        print(f"{i:2d}. [{rec['company']}] {rec['title'][:40]:40s} (ìœ ì‚¬ë„: {rec['similarity']:.3f})")
    
    baseline_diversity = reranker.analyze_diversity(baseline, job_embeddings)
    print(f"\në‹¤ì–‘ì„± ë¶„ì„:")
    print(f"  - í‰ê·  ìœ ì‚¬ë„: {baseline_diversity['avg_similarity']:.3f}")
    print(f"  - ë‹¤ì–‘ì„± ì ìˆ˜: {baseline_diversity['diversity_score']:.3f}")
    
    # 2. MMR ì¬ì •ë ¬
    print("\n" + "=" * 60)
    print("2. MMR ì¬ì •ë ¬ (lambda=0.5, ë‹¤ì–‘ì„± ì¤‘ì‹œ)")
    print("=" * 60)
    mmr_results = reranker.mmr_rerank(
        recommendations=recommendations[:30],
        resume_embedding=resume_embedding,
        job_embeddings=job_embeddings,
        lambda_param=0.5,
        top_n=10
    )
    
    for i, rec in enumerate(mmr_results, 1):
        print(f"{i:2d}. [{rec['company']}] {rec['title'][:40]:40s} (ìœ ì‚¬ë„: {rec['similarity']:.3f})")
    
    mmr_diversity = reranker.analyze_diversity(mmr_results, job_embeddings)
    print(f"\në‹¤ì–‘ì„± ë¶„ì„:")
    print(f"  - í‰ê·  ìœ ì‚¬ë„: {mmr_diversity['avg_similarity']:.3f}")
    print(f"  - ë‹¤ì–‘ì„± ì ìˆ˜: {mmr_diversity['diversity_score']:.3f}")
    print(f"  - ë‹¤ì–‘ì„± ê°œì„ : {(mmr_diversity['diversity_score'] - baseline_diversity['diversity_score']) * 100:.1f}%")
    
    # 3. ì°¸ì‹ ì„± ì ìˆ˜ ì¶”ê°€
    print("\n" + "=" * 60)
    print("3. ì°¸ì‹ ì„± ì ìˆ˜ ê³„ì‚°")
    print("=" * 60)
    novelty_results = reranker.calculate_novelty_scores(
        recommendations=recommendations[:10],
        user_id=test_resume_id
    )
    
    for i, rec in enumerate(novelty_results, 1):
        print(f"{i:2d}. [{rec['company']}] {rec['title'][:40]:40s}")
        print(f"     ìœ ì‚¬ë„: {rec['similarity']:.3f}, ì°¸ì‹ ì„±: {rec['novelty_score']:.3f}")
    
    # 4. í•˜ì´ë¸Œë¦¬ë“œ ì¬ì •ë ¬
    print("\n" + "=" * 60)
    print("4. í•˜ì´ë¸Œë¦¬ë“œ ì¬ì •ë ¬ (ë‹¤ì–‘ì„± + ì°¸ì‹ ì„± + ì—°ê´€ì„±)")
    print("=" * 60)
    hybrid_results = reranker.hybrid_rerank(
        recommendations=recommendations[:30],
        resume_embedding=resume_embedding,
        job_embeddings=job_embeddings,
        user_id=test_resume_id,
        diversity_weight=0.3,
        novelty_weight=0.2,
        relevance_weight=0.5,
        top_n=10
    )
    
    for i, rec in enumerate(hybrid_results, 1):
        print(f"{i:2d}. [{rec['company']}] {rec['title'][:40]:40s}")
        print(f"     ìœ ì‚¬ë„: {rec['similarity']:.3f}, ì°¸ì‹ ì„±: {rec['novelty_score']:.3f}, ìµœì¢…ì ìˆ˜: {rec['final_score']:.3f}")
    
    hybrid_diversity = reranker.analyze_diversity(hybrid_results, job_embeddings)
    print(f"\në‹¤ì–‘ì„± ë¶„ì„:")
    print(f"  - í‰ê·  ìœ ì‚¬ë„: {hybrid_diversity['avg_similarity']:.3f}")
    print(f"  - ë‹¤ì–‘ì„± ì ìˆ˜: {hybrid_diversity['diversity_score']:.3f}")
    print(f"  - ë‹¤ì–‘ì„± ê°œì„ : {(hybrid_diversity['diversity_score'] - baseline_diversity['diversity_score']) * 100:.1f}%")
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
